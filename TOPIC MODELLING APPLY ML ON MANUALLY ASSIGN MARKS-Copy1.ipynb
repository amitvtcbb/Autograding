{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0858ff9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import re\n",
    "warnings.filterwarnings(\"ignore\", category = DeprecationWarning)\n",
    "#loading libraries \n",
    "import gensim\n",
    "from gensim.models import Phrases\n",
    "from gensim.models.phrases import Phraser\n",
    "from gensim import models\n",
    "from gensim.models.phrases import Phrases, ENGLISH_CONNECTOR_WORDS\n",
    "from gensim import corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import TfidfModel\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "import nltk\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from spellchecker import SpellChecker\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import FreqDist\n",
    " \n",
    "from gensim.models.phrases import Phrases, ENGLISH_CONNECTOR_WORDS\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "import wordcloud\n",
    "\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d10d203b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('G:\\\\GitHub Classroom Assistant\\\\Topic Modelling Dataset\\\\TMC\\\\CASSIGNMENT-1.csv',encoding='unicode_escape')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a292670e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Sr No.', 'Name', 'Roll No', 'Gender', 'Email Address', 'Timestamp',\n",
       "       'Find out errors(if any), write solution and also give explanation of the following code: \\n#include <stdio.h>  \\nint main()  \\n{      \\n           a = 10;      \\n           printf(\"The value of a is : %d\", a);     \\n           return 0;  \\n}  '],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e57fe8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns=['No.','Name','ID','Gender','Email Address','Timestatmp','Comment1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea3170bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ee93f5",
   "metadata": {},
   "source": [
    "## <font color=\"Blue\"> REMOVE ROW WITH BLANK CELL</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b573c27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeblankrow(data,blank_row_info,col):\n",
    "    \n",
    "    print(f\"Total number of row befor cleaning : {len(data)}\")\n",
    "    # Store the rows with empty lists in the specified column\n",
    "    blank_rows = data[data[col].apply(lambda x: len(x) == 0)]\n",
    "    \n",
    "    # Extract the index, student ID, name, and gender of the blank rows\n",
    "    blank_rows_info = blank_rows[['ID', 'Name', 'Gender','Comment1']]\n",
    "    \n",
    "    # Remove rows with empty lists in the specified column\n",
    "    data = data[data[col].apply(lambda x: len(x) > 0)]\n",
    "    \n",
    "    # Reset index if needed\n",
    "    data.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    print(f\"\\n Total Number of row after cleaning :{len(data)}\") \n",
    "    print(f\"\\n Total Number of blank row is :{len(blank_rows_info)}\")\n",
    "    \n",
    "    print(f\"\\n{blank_rows_info}\")\n",
    "    \n",
    "    return data,blank_row_info\n",
    "\n",
    "# Example usage:\n",
    "# new_data, blank_rows_info = remove_blank_row(data, 'cleancomment3')\n",
    "# print(blank_rows_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1bc9525e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of row befor cleaning : 283\n",
      "\n",
      " Total Number of row after cleaning :283\n",
      "\n",
      " Total Number of blank row is :0\n",
      "\n",
      "Empty DataFrame\n",
      "Columns: [ID, Name, Gender, Comment1]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "blankrow=pd.DataFrame()\n",
    "data,blankrow=removeblankrow(data,blankrow,\"Comment1\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b61cf9",
   "metadata": {},
   "source": [
    "## <font color=\"blue\"> CLEANING DATASET </FONT>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "adb8f46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_keywords = [\n",
    "    \"auto\", \"break\", \"case\", \"char\", \"const\", \"continue\", \"default\", \"do\", \"double\",\n",
    "    \"else\", \"enum\", \"extern\", \"float\", \"for\", \"goto\", \"if\", \"int\", \"long\", \"register\",\n",
    "    \"return\", \"short\", \"signed\", \"sizeof\", \"static\", \"struct\", \"switch\", \"typedef\",\n",
    "    \"union\", \"unsigned\", \"void\", \"volatile\", \"while\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e848097f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert all comments in lower case\n",
    "data['Comment1']=df['Comment1'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ee389117",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_answer=\"Error: There is compile-time error. Variable is not declare. Solution: Variable must be declare before use. Here we have to declare it integer.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "674d6373",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:17: DeprecationWarning: invalid escape sequence '\\S'\n",
      "<>:17: DeprecationWarning: invalid escape sequence '\\S'\n",
      "C:\\Users\\Amit\\AppData\\Local\\Temp\\ipykernel_9084\\2927420580.py:17: DeprecationWarning: invalid escape sequence '\\S'\n",
      "  comment = re.sub(\"\\S*\\d\\S*\", \"\", comment).strip() # remove word which contain digit in it.\n"
     ]
    }
   ],
   "source": [
    "def clean_comment(comment):\n",
    "    \"\"\"Cleans student comments, removing C programming statements and optionally applying other preprocessing steps.\"\"\"\n",
    "    \n",
    "    # Remove C programming statements\n",
    "    comment = re.sub(r\"#.*\", \"\", comment) # Remove header files\n",
    "    comment = re.sub(r\"(void|int|char|float|double)\\s+[\\w_]+\\s*\\([^)]*\\)[\\s\\{]\", \"\", comment)  # Remove function declarations\n",
    "    comment = re.sub(r\"\\{[\\s\\S]*?\\}\", \"\", comment)  # Remove code blocks within curly braces\n",
    "\n",
    "    comment = re.sub(r\"[\\=+:\\-*/%\\(\\)\\{\\}\\[\\];\\\"\\'`<,>,.,0-9]\", \" \", comment)\n",
    "\n",
    "   \n",
    "#     for keyword in c_keywords:\n",
    "#         comment = re.sub(rf\"\\b{keyword} \", \"\", comment) \n",
    "           \n",
    "    comment = re.sub(r\"\\b\\w{1,2}\\b\", \"\", comment)  # Remove single- or two-letter words\n",
    "    comment = re.sub(r\"\\s+\", \" \", comment)  # Remove extra whitespace\n",
    "    comment = re.sub(\"\\S*\\d\\S*\", \"\", comment).strip() # remove word which contain digit in it.\n",
    "\n",
    "\n",
    "    return comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5ba51546",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"CleanComment\"]=data['Comment1'].apply(clean_comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3ec9372d",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_answer=teacher_answer.lower()\n",
    "clean_teacher_answer=clean_comment(teacher_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "46b79deb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'error there compile time error variable not declare solution variable must declare before use here have declare integer'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_teacher_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9fb6f9ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of row befor cleaning : 283\n",
      "\n",
      " Total Number of row after cleaning :266\n",
      "\n",
      " Total Number of blank row is :17\n",
      "\n",
      "            ID                              Name  Gender  \\\n",
      "17    23BCA286             SIRVI VIKAS POKARRAM     Male   \n",
      "26    23BCA107           Mistry sachi Umeshbhai   Female   \n",
      "34    23BCA078             Lad Trisa Mukeshbhai   Female   \n",
      "40    23BCA260           Reema Umeshbhai sahani   Female   \n",
      "60    23bca258          RATHOD VICKY SHARADBHAI     Male   \n",
      "72    23BCA316              YADAV SONAL RAMGYAN   Female   \n",
      "101   23BCA034     Das Vihangkumar Abhijeetbhai     Male   \n",
      "111   23BCA015           BHAVSAR PURV HIRENBHAI     Male   \n",
      "124     BCA053               Tushar Anil Jadhav     Male   \n",
      "177   23BCA023  KRINCEKUMAR NAVINBHAI CHAUDHARI     Male   \n",
      "178   23bca005           Ahir kheyan Nileshbhai     Male   \n",
      "242   23BCA077              Lad meet ishvarbhai     Male   \n",
      "253  23BCA178                       Patel misha   Female   \n",
      "261   23BCA315             Yadav shivam ramsakal    Male   \n",
      "270   23BCA278        SHIKARVAR PRINSI MUNNASINH  Female   \n",
      "274   23BCA027              CHAUDHARI YASH RASIK    Male   \n",
      "277   23BCA160                     Patel Heer K.  Female   \n",
      "\n",
      "                                              Comment1  \n",
      "17   \\n#include <stdio.h>\\n#include <conio.h>\\nint ...  \n",
      "26                                                  10  \n",
      "34                                                   5  \n",
      "40   # include<stdio.h>\\n#include<conio.h>\\nint mai...  \n",
      "60                                                  10  \n",
      "72   #include<studio.h>\\n#include<conio.h>\\nint mai...  \n",
      "101  #include <stdio.h>int main(){ int a = 10; // d...  \n",
      "111  #include<stdio.h>\\n#include<conio.h>\\nint main...  \n",
      "124  #include<studio.h>\\nint main()\\n{\\n      int a...  \n",
      "177  #include <stdio.h>  \\nint main()  \\n{      \\n ...  \n",
      "178  #include <stdio.h>  \\nint main()  \\n{      \\n ...  \n",
      "242                                                 10  \n",
      "253                                                  0  \n",
      "261  #include <stdio.h>\\n\\nint main() {\\n    int a ...  \n",
      "270  #include <conio.h>\\n#include<studio.h>\\nint ma...  \n",
      "274                                                 10  \n",
      "277  #include<stdio,h>\\n#include<conio.h>\\nvoid mai...  \n"
     ]
    }
   ],
   "source": [
    "data,blankrow=removeblankrow(data,blankrow,\"CleanComment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9ea7dbe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 266 entries, 0 to 265\n",
      "Data columns (total 8 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   No.            266 non-null    int64 \n",
      " 1   Name           266 non-null    object\n",
      " 2   ID             266 non-null    object\n",
      " 3   Gender         266 non-null    object\n",
      " 4   Email Address  266 non-null    object\n",
      " 5   Timestatmp     266 non-null    object\n",
      " 6   Comment1       266 non-null    object\n",
      " 7   CleanComment   266 non-null    object\n",
      "dtypes: int64(1), object(7)\n",
      "memory usage: 16.8+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3062892",
   "metadata": {},
   "source": [
    "We find that from 283 there is 257 records remaining from the above output. \n",
    "\n",
    "so 283-257 = 26 students comments are remove after clearning the comment.\n",
    "\n",
    "#### CleanComment : Contain the comment after removing stopwords, programming keywords, Programming Statement, Remove multiple spaces, digits etc.."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0101621",
   "metadata": {},
   "source": [
    "### <font color=\"blue\"> FIND SYNONYMES </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "035907c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Python310\\lib\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a4ea07b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synonyms of 'error': ['wrongdoing', 'erroneous_belief', 'mistake', 'misplay', 'fault', 'erroneousness', 'computer_error', 'error']\n",
      "Synonyms of 'solution': ['resolution', 'solution', 'answer', 'root', 'result', 'solvent']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "import pandas as pd\n",
    "\n",
    "# Function to get synonyms of a word using WordNet\n",
    "def get_synonyms(word):\n",
    "    synonyms = []\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            synonyms.append(lemma.name())\n",
    "    return synonyms\n",
    "\n",
    "# Load your DataFrame 'data' containing comments in 'Cleancomment' column\n",
    "# Assuming 'data' is already loaded\n",
    "\n",
    "# Initialize lists to store synonyms of 'error' and 'solution'\n",
    "error_synonyms_list = []\n",
    "solution_synonyms_list = []\n",
    "\n",
    "# Iterate over each comment in the DataFrame\n",
    "for comment in data['CleanComment']:\n",
    "    # Find synonyms of 'error' and 'solution' in the comment\n",
    "    error_synonyms = get_synonyms('error')\n",
    "    solution_synonyms = get_synonyms('solution')\n",
    "    \n",
    "    # Store synonyms in the lists\n",
    "    error_synonyms_list.extend(error_synonyms)\n",
    "    solution_synonyms_list.extend(solution_synonyms)\n",
    "\n",
    "# Remove duplicates from the lists\n",
    "error_synonyms_list = list(set(error_synonyms_list))\n",
    "solution_synonyms_list = list(set(solution_synonyms_list))\n",
    "\n",
    "# Print and store synonyms in a list\n",
    "print(\"Synonyms of 'error':\", error_synonyms_list)\n",
    "print(\"Synonyms of 'solution':\", solution_synonyms_list)\n",
    "\n",
    "# Optionally, you can store the synonyms lists in your DataFrame or elsewhere for later use\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "596ef51e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the code you provided missing declaration for the variable fix this error you should add line the beginning the main function that declares integer the corrected code would look like this this code declares integer and initializes with the value then prints the value using the printf function finally the main function returns indicate that the program has run successfully'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"CleanComment\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31307cbb",
   "metadata": {},
   "source": [
    "### <font color=\"blue\"> REMOVE STOPWORDS </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "87fd3eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.corpus import stopwords\n",
    "# from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Load English stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Function to remove stopwords and store them in a text file\n",
    "def remove_stopwords_and_store(comment):\n",
    "    # Tokenize the comment\n",
    "    words = word_tokenize(comment)\n",
    "    \n",
    "    # Remove stopwords and store them in a set\n",
    "    removed_stopwords = [word for word in words if word.lower() not in stop_words]\n",
    "    \n",
    "    # Convert removed stopwords to a string\n",
    "    removed_stopwords_str = ' '.join(removed_stopwords)\n",
    "    \n",
    "    # Store removed stopwords in a text file\n",
    "    with open('removed_stopwords.txt', 'a') as file:\n",
    "        file.write(' '.join(set(words) - set(removed_stopwords)) + '\\n')\n",
    "    \n",
    "    return removed_stopwords_str\n",
    "\n",
    "# Apply the function to each comment in the dataframe and store the result in a new column\n",
    "data[\"comment_without_stopword\"] = data[\"CleanComment\"].apply(remove_stopwords_and_store)\n",
    "lem_teacher_answer=remove_stopwords_and_store(clean_teacher_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0f0f5db6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'code provided missing declaration variable fix error add line beginning main function declares integer corrected code would look like code declares integer initializes value prints value using printf function finally main function returns indicate program run successfully'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"comment_without_stopword\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0ff810fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'error: there is compile-time error. variable is not declare. solution: variable must be declare before use. here we have to declare it integer.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teacher_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a8d4a872",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'error compile time error variable declare solution variable must declare use declare integer'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lem_teacher_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "02a8f423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop_words = stopwords.words('english')\n",
    "# data[\"comment_without_stopword\"] = data[\"CleanComment\"].apply(lambda x:' '.join([word for word in word_tokenize(x) if word not in stop_words]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9a59ef51",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Comment_Length\"]=data[\"CleanComment\"].apply(lambda x:len(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72813d4",
   "metadata": {},
   "source": [
    "### <font color=\"blue\"> APPLY BIGRAM TRIGRAM BEFORE REMOVE STOP WORDS </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c0ce5ebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. (main, function) - Count: 28\n",
      "2. (declares, integer) - Count: 28\n",
      "3. (corrected, code) - Count: 41\n",
      "4. (initializes, value) - Count: 15\n",
      "5. (prints, value) - Count: 25\n",
      "6. (value, using) - Count: 40\n",
      "7. (using, printf) - Count: 78\n",
      "8. (printf, function) - Count: 27\n",
      "9. (syntax, error) - Count: 21\n",
      "10. (declaration, error) - Count: 12\n",
      "11. (variable, used) - Count: 76\n",
      "12. (used, without) - Count: 85\n",
      "13. (without, declaration) - Count: 56\n",
      "14. (errors, code) - Count: 12\n",
      "15. (variable, declared) - Count: 51\n",
      "16. (data, type) - Count: 101\n",
      "17. (need, declare) - Count: 33\n",
      "18. (declare, variable) - Count: 47\n",
      "19. (assigning, value) - Count: 14\n",
      "20. (error, variable) - Count: 41\n",
      "21. (solution, int) - Count: 30\n",
      "22. (explanation, added) - Count: 21\n",
      "23. (added, int) - Count: 75\n",
      "24. (int, declare) - Count: 75\n",
      "25. (variable, integer) - Count: 30\n",
      "26. (integer, initialized) - Count: 17\n",
      "27. (initialized, value) - Count: 23\n",
      "28. (value, printed) - Count: 18\n",
      "29. (printed, value) - Count: 16\n",
      "30. (printf, correct) - Count: 13\n",
      "31. (correct, format) - Count: 15\n",
      "32. (format, specifier) - Count: 38\n",
      "33. (declare, integer) - Count: 63\n",
      "34. (errors, provided) - Count: 15\n",
      "35. (provided, code) - Count: 21\n",
      "36. (code, appears) - Count: 15\n",
      "37. (appears, variable) - Count: 15\n",
      "38. (without, declared) - Count: 26\n",
      "39. (declared, variables) - Count: 13\n",
      "40. (variables, need) - Count: 20\n",
      "41. (need, declared) - Count: 23\n",
      "42. (declared, use) - Count: 21\n",
      "43. (use, corrected) - Count: 12\n",
      "44. (error, solution) - Count: 22\n",
      "45. (type, solution) - Count: 16\n",
      "46. (variable, declaration) - Count: 13\n",
      "47. (code, variable) - Count: 12\n",
      "48. (code, explanation) - Count: 12\n",
      "49. (int, main) - Count: 12\n",
      "50. (integer, variable) - Count: 64\n",
      "51. (variable, named) - Count: 11\n",
      "52. (value, printf) - Count: 11\n",
      "53. (value, variable) - Count: 55\n",
      "54. (variable, using) - Count: 16\n",
      "55. (declare, data) - Count: 12\n",
      "56. (integer, using) - Count: 16\n",
      "57. (using, int) - Count: 18\n",
      "58. (line, int) - Count: 12\n",
      "59. (include, data) - Count: 46\n",
      "60. (type, variable) - Count: 54\n",
      "61. (variable, code) - Count: 44\n",
      "62. (print, value) - Count: 21\n",
      "63. (error, syntax) - Count: 14\n",
      "64. (declared, data) - Count: 13\n",
      "65. (type, int) - Count: 12\n",
      "66. (error, code) - Count: 12\n",
      "67. (declared, used) - Count: 24\n",
      "68. (assigns, value) - Count: 14\n",
      "69. (undeclared, identifier) - Count: 11\n",
      "70. (errors, variable) - Count: 19\n",
      "71. (declaration, need) - Count: 41\n",
      "72. (need, include) - Count: 44\n",
      "73. (variable, solution) - Count: 40\n",
      "74. (solution, added) - Count: 42\n",
      "75. (int, int) - Count: 44\n",
      "76. (variable, ensures) - Count: 36\n",
      "77. (ensures, proper) - Count: 43\n",
      "78. (proper, usage) - Count: 42\n",
      "79. (usage, variable) - Count: 42\n",
      "80. (code, correctly) - Count: 41\n",
      "81. (correctly, assign) - Count: 40\n",
      "82. (assign, value) - Count: 46\n",
      "83. (variable, print) - Count: 39\n",
      "84. (print, using) - Count: 41\n",
      "85. (declared, variable) - Count: 12\n",
      "86. (used, need) - Count: 15\n",
      "Similar phrases with counts have been stored in the CSV file: G:\\GitHub Classroom Assistant\\Topic Modelling Dataset\\TMC\\similar_phrases_with_counts.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import itertools\n",
    "\n",
    "def find_similar_phrases(data):\n",
    "    # Initialize a dictionary to store counts of word pairs\n",
    "    word_pairs_count = {}\n",
    "    \n",
    "    # Iterate over each comment in the data\n",
    "    for comment in data['comment_without_stopword']:\n",
    "        # Split the comment into words\n",
    "        words = comment.split()\n",
    "        \n",
    "        # Generate all pairs of adjacent words\n",
    "        pairs = list(zip(words, words[1:]))\n",
    "        \n",
    "        # Update the word_pairs_count dictionary\n",
    "        for pair in pairs:\n",
    "            word_pairs_count[pair] = word_pairs_count.get(pair, 0) + 1\n",
    "    \n",
    "    # Filter pairs where both words are alphabetic and have a count greater than a threshold\n",
    "    similar_phrases = [(pair[0], pair[1], count) for pair, count in word_pairs_count.items() if count > threshold\n",
    "                       and pair[0].isalpha() and pair[1].isalpha()]\n",
    "    \n",
    "    return similar_phrases\n",
    "\n",
    "# Set the threshold for considering word pairs\n",
    "threshold = 10\n",
    "\n",
    "# Find similar phrases in the data\n",
    "similar_phrases = find_similar_phrases(data)\n",
    "\n",
    "# Print the list of similar phrases in the desired format\n",
    "for i, (word1, word2, count) in enumerate(similar_phrases, start=1):\n",
    "    print(f\"{i}. ({word1}, {word2}) - Count: {count}\")\n",
    "\n",
    "# Store the list of similar phrases and their occurrences in a CSV file\n",
    "output_file = \"G:\\\\GitHub Classroom Assistant\\\\Topic Modelling Dataset\\\\TMC\\\\similar_phrases_with_counts.csv\"\n",
    "with open(output_file, 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow([\"Index\", \"Word1\", \"Word2\", \"Count\"])\n",
    "    for i, (word1, word2, count) in enumerate(similar_phrases, start=1):\n",
    "        writer.writerow([i, word1, word2, count])\n",
    "\n",
    "print(\"Similar phrases with counts have been stored in the CSV file:\", output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71aaf5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def find_pairs_with_word(similar_phrases, target_word):\n",
    "    pairs_containing_word = []\n",
    "    \n",
    "    # Find all pairs containing the target word\n",
    "    for phrase_pair in similar_phrases:\n",
    "        if target_word in phrase_pair:\n",
    "            pairs_containing_word.append(phrase_pair)\n",
    "    \n",
    "    # Remove counts from the pairs\n",
    "    pairs_containing_word = [(word1, word2) for word1, word2, _ in pairs_containing_word]\n",
    "    \n",
    "    return pairs_containing_word\n",
    "\n",
    "def combine_phrases(pair):\n",
    "    return ''.join(pair)\n",
    "\n",
    "# Ask the user for the target word\n",
    "target_word = input(\"Enter the word to search for: \")\n",
    "\n",
    "# Find pairs containing the target word\n",
    "pairs_containing_target_word = find_pairs_with_word(similar_phrases, target_word)\n",
    "\n",
    "# Print the pairs containing the target word\n",
    "print(\"Pairs containing '{}':\".format(target_word))\n",
    "for i, phrase_pair in enumerate(pairs_containing_target_word):\n",
    "    print(\"{}. {}\".format(i+1, ' '.join(phrase_pair)))\n",
    "\n",
    "# If no pairs found, inform the user and exit\n",
    "if not pairs_containing_target_word:\n",
    "    print(\"No pairs found containing '{}'.\".format(target_word))\n",
    "else:\n",
    "    # Ask the user for their choice of pair\n",
    "    choice = input(\"Enter the number of the pair to combine: \")\n",
    "    try:\n",
    "        choice_index = int(choice) - 1\n",
    "        chosen_pair = pairs_containing_target_word[choice_index]\n",
    "        \n",
    "        # Combine the chosen pair\n",
    "        combined_phrase = combine_phrases(chosen_pair)\n",
    "        print(\"Combined phrase:\", combined_phrase)\n",
    "        \n",
    "        # Apply the combined phrase to the dataset\n",
    "        data['apply_bigram_trigram'] = data['comment_without_stopword'].apply(lambda x: x.replace(' '.join(chosen_pair), combined_phrase))\n",
    "        \n",
    "        # Store the details where changes occurred\n",
    "        combine_word = data[data['apply_bigram_trigram'].str.contains(combined_phrase)]\n",
    "        combine_word.reset_index(drop=True, inplace=True)\n",
    "        \n",
    "        # Save the dataframe with combined words\n",
    "        combine_word.to_csv('G:\\\\GitHub Classroom Assistant\\\\Topic Modelling Dataset\\\\TMC\\\\combine_word_ass1.csv', index=False)\n",
    "        \n",
    "        # Save the updated original dataframe\n",
    "#         data.to_csv('data_with_changes.csv', index=False)\n",
    "        \n",
    "        print(\"Changes applied and stored in separate dataframes successfully.\")\n",
    "    except (ValueError, IndexError):\n",
    "        print(\"Invalid choice. Please enter a valid number.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b00b18f",
   "metadata": {},
   "source": [
    "### <font color=\"blue\"> VISUALIZE WORD FREQUENCY AFTER REMOVING STOPWORDS </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35250667",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_comments = [word_tokenize(comment) for comment in data[\"apply_bigram_trigram\"]]\n",
    "\n",
    "# Flatten the list of tokenized comments\n",
    "flat_tokens = [token for sublist in tokenized_comments for token in sublist ] #if len(token)>4]\n",
    "\n",
    "# Create a frequency distribution\n",
    "freq_dist = FreqDist(flat_tokens)\n",
    "\n",
    "# Display the most common words and their frequencies\n",
    "print(\"Top 20 Most Common Words:\")\n",
    "print(freq_dist.most_common(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17ded8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.rename(columns = {'apply_bigram_trigram':'comment_after_bitrigram'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8377ab58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "#top_words = word_freq.most_common(20)\n",
    "x, y = zip(*freq_dist.most_common(20) )\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(x, y)\n",
    "plt.title('Top 20 Most Common Words (Before Lemmatization)')\n",
    "plt.xlabel('Words')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2dfe40c",
   "metadata": {},
   "source": [
    "### DICTIONARY (CORPUS) OF ALL UNIQUE WORD USED IN COMMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2c6335",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Extract all unique words from the 'comment_after_bitrigram' column\n",
    "unique_words = set()\n",
    "for comment in data['comment_after_bitrigram']:\n",
    "    # Split the comment into words and remove symbols and special characters\n",
    "    words = re.findall(r'\\b\\w+\\b', comment)\n",
    "    unique_words.update(words)\n",
    "\n",
    "# Convert the set of unique words to a list\n",
    "unique_words_list = list(unique_words)\n",
    "\n",
    "# Write the unique words to a text file with UTF-8 encoding\n",
    "file_path = r'G:\\GitHub Classroom Assistant\\Topic Modelling Dataset\\TMC\\CORPUS\\WORDS.TXT'\n",
    "with open(file_path, 'w', encoding='utf-8') as file:\n",
    "    for word in unique_words_list:\n",
    "        file.write(word + '\\n')\n",
    "\n",
    "print(\"Total number of unique words:\", len(unique_words_list))\n",
    "print(\"Unique words have been written to:\", file_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0e5050",
   "metadata": {},
   "source": [
    "### <font color=\"blue\"> SPELL CHECK USING \"SPELLCHECKER\" PACKAGES.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9f5e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "# Initialize SpellChecker with English language\n",
    "spell = SpellChecker(language='en')\n",
    "\n",
    "# Define words to exclude from correction\n",
    "excluded_words = {'scanf', 'printf', 'getch', 'stdio', 'conio', 'clrscr', 'getchar',  \n",
    "                  'clrscr', 'void', 'int', 'datatype','runtime'}\n",
    "\n",
    "# Add excluded_words in spellchecker directory.\n",
    "\n",
    "# Define manual corrections for specific words\n",
    "manual_corrections = {'flot': 'float', 'lot':'float','clrsccr': 'clrscr','flost':'float','\\x91clrscr':'clrscr'}\n",
    "\n",
    "# Function to spellcheck each comment\n",
    "def spellcheck_comment(comment):\n",
    "    if comment is None or comment == \"\":\n",
    "        return \"\", {}, 0\n",
    "\n",
    "    # Tokenize the comment\n",
    "    tokens = comment.split()\n",
    "\n",
    "    # Initialize dictionary to store misspelled words and their corrections\n",
    "    misspellings = {}\n",
    "\n",
    "    # Iterate through each word in the comment\n",
    "    for word in tokens:\n",
    "        # Check if the word is in the excluded list\n",
    "        if word.lower() in excluded_words:\n",
    "            continue  # Skip correction for excluded words\n",
    "        \n",
    "        # Check if the word is misspelled\n",
    "        corrected_word = spell.correction(word)\n",
    "        \n",
    "        # Check if the corrected word is one of the manual corrections\n",
    "        if corrected_word in manual_corrections:\n",
    "            corrected_word = manual_corrections[corrected_word]\n",
    "        \n",
    "        if corrected_word != word:\n",
    "            # Store the misspelled word and its corrected form in the dictionary\n",
    "            misspellings[word] = corrected_word\n",
    "\n",
    "    # Replace misspelled words with correct spelling in the comment\n",
    "    corrected_tokens = []\n",
    "    total_incorrect_words = 0\n",
    "    for word in tokens:\n",
    "        corrected_word = misspellings.get(word)\n",
    "        if corrected_word is not None:\n",
    "            corrected_tokens.append(corrected_word)\n",
    "            total_incorrect_words += 1\n",
    "        else:\n",
    "            corrected_tokens.append(word)\n",
    "    \n",
    "    corrected_comment = ' '.join(corrected_tokens)\n",
    "\n",
    "    return corrected_comment, misspellings, total_incorrect_words\n",
    "\n",
    "# Apply spellchecking to each comment in the dataframe\n",
    "data['CorrectComment_Spell'], data['misspellings'], data['total_incorrect_words'] = zip(*data['comment_after_bitrigram'].apply(spellcheck_comment))\n",
    "\n",
    "# Create a dictionary of incorrect spellings against correct spellings\n",
    "incorrect_correct_dict = {}\n",
    "for misspelling_dict in data['misspellings']:\n",
    "    for misspelled_word, corrected_word in misspelling_dict.items():\n",
    "        if misspelled_word not in incorrect_correct_dict:\n",
    "            incorrect_correct_dict[misspelled_word] = corrected_word\n",
    "\n",
    "# Create a new dataframe with required columns\n",
    "new_data = data[['ID', 'Name', 'Comment1', 'comment_without_stopword', 'misspellings', 'CorrectComment_Spell', 'total_incorrect_words']]\n",
    "\n",
    "# Display the corrected comments and misspelled words\n",
    "print(new_data)\n",
    "\n",
    "# Display the dictionary of incorrect spellings against correct spellings\n",
    "print(\"Dictionary of incorrect spellings against correct spellings:\")\n",
    "print(incorrect_correct_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b443b3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data[['ID','comment_without_stopword','misspellings','CorrectComment_Spell','total_incorrect_words']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab88b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data[new_data[\"total_incorrect_words\"]>0][\"misspellings\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a45444",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Filter rows where total_incorrect_words > 0\n",
    "filtered_data = new_data[new_data['total_incorrect_words'] > 0]\n",
    "filtered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390f72d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3580c3d5",
   "metadata": {},
   "source": [
    "### <font color=\"blue\"> STORE COMMENT OF WORDLENGTH 3 AND WORDLENGTH 4 IN DIFFERENT COLUMN</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a322cc9",
   "metadata": {},
   "source": [
    "We cannot remove word which length is <3. Now We are going to do this. We try to remove wordlength < 3 and <4 and store in other columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e841d3e",
   "metadata": {},
   "source": [
    "- We store comment of word length 3 and 4 in different column.\n",
    "- So We can check what is the effect if we remove word of 3 character long and 4 character long against all length word.\n",
    "- It also say effect of word in our vocabulary. \n",
    "- We also count total words in comment so we can bifergate comment in basic vs detail explaination"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f207653d",
   "metadata": {},
   "source": [
    "## CREATE COLUMN CleanComment3 , CleanComment4, TotalWordgte3, TotalWordgt4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdabf572",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to filter comments based on word length\n",
    "def filter_comments(comment, word_length):\n",
    "    return ' '.join([word for word in comment.split() if len(word) >= word_length])\n",
    "\n",
    "# Function to count total words in comments based \n",
    "def count_words(comment):\n",
    "    return len(comment.split())\n",
    "\n",
    "\n",
    "\n",
    "# Word lengths to consider\n",
    "word_lengths = [3, 4]  # Adjust as needed\n",
    "\n",
    "# Create new columns for each word length\n",
    "for length in word_lengths:\n",
    "    column_name = f'CleanComment{length}'\n",
    "    data[column_name] = data['CorrectComment_Spell'].apply(lambda x: filter_comments(x, length))\n",
    "    \n",
    "for length in word_lengths:\n",
    "    word_length=f'TotalWordgte{length}'\n",
    "    column_name = f'CleanComment{length}'\n",
    "    data[word_length] = data[column_name].apply(lambda x: count_words(x))\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5381c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "# # Assuming you have a DataFrame called 'data' with columns 'Comment' and 'Comment_Length'\n",
    "# # 'Comment' contains the text of the comments, and 'Comment_Length' contains the length of each comment\n",
    "# # 'Type' contains the labels indicating whether the comment is \"detail\" (1) or \"basic\" (0)\n",
    "\n",
    "# # Step 1: Split the dataset into features (X) and target variable (y)\n",
    "# X = data['Comment_Length'].values.reshape(-1, 1)  # Reshape to fit the model input format\n",
    "# y = data['Type']\n",
    "\n",
    "# # Step 2: Split the dataset into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Step 3: Choose and train a machine learning model\n",
    "# model = LogisticRegression()\n",
    "# model.fit(X_train, y_train)\n",
    "\n",
    "# # Step 4: Evaluate the model's performance\n",
    "# y_pred = model.predict(X_test)\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "# print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Step 5: Use the trained model for prediction (optional)\n",
    "# new_comment_length = 60  # Example length of a new comment\n",
    "# predicted_type = model.predict([[new_comment_length]])\n",
    "# print(\"Predicted Type:\", predicted_type)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0caa1803",
   "metadata": {},
   "source": [
    "### <font color=\"blue\">Differentiate \"Basic\" and \"Detailed\" Comment</blue>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e256ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def comment_type(x):\n",
    "    if len(x) > 50 :\n",
    "        return \"Detail\"\n",
    "    else:\n",
    "        return \"Basic\"\n",
    "    \n",
    "# Word lengths to consider\n",
    "word_lengths = [3, 4]  # Adjust as needed\n",
    "\n",
    "for length in word_lengths:\n",
    "    comment=f'ComemntType{length}'\n",
    "    column_name = f'CleanComment{length}'\n",
    "    data[f'{column_name}_Length']=data[column_name].apply(lambda x: len(x))\n",
    "    data[comment] = data[column_name].apply(lambda x: comment_type(x))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd47c95",
   "metadata": {},
   "source": [
    "#### CleanComment3 : Contain comment after removing word legnth < 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf5e076",
   "metadata": {},
   "source": [
    "#### TotalWordge3: Contain total word in CleanComment3. Useful to find comment is basic or detail."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3a4b05",
   "metadata": {},
   "source": [
    "#### CleanComment4: Contain comment after removing word length < 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804bff2a",
   "metadata": {},
   "source": [
    "#### TotalWordge4: Contain total word in CleanComment4. Useful to find comment is basic or detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37761ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.rename(columns = {'ComemntType3':'CommentType3','ComemntType4':'CommentType4'}, inplace = True) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bcb00f1",
   "metadata": {},
   "source": [
    "- CleanComment column with number contain the comment after removing stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b954410c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"CommentType3\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2183338f",
   "metadata": {},
   "source": [
    "### <font color=\"blue\">Perform Wordfrequency chart against Word Length. </font>\n",
    "#### So it show importance of word and why we need to remove word whose length is < 3 character long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324bafbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to count word frequency for a given column\n",
    "def count_word_frequency(column):\n",
    "    word_length_counts = column.str.split().explode().str.len().value_counts().sort_index()\n",
    "    return word_length_counts\n",
    "# Plot word frequency against word length for each column in one row\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5), sharey=True)\n",
    "\n",
    "# Column names\n",
    "column_names = ['CleanComment', 'CleanComment3', 'CleanComment4']\n",
    "    \n",
    "for i, column_name in enumerate(column_names):\n",
    "        word_length_counts = count_word_frequency(data[column_name])\n",
    "        axes[i].bar(word_length_counts.index, word_length_counts.values, color='skyblue')\n",
    "        axes[i].set_title(column_name)\n",
    "        axes[i].set_xlabel('Word Length')\n",
    "        axes[i].set_ylabel('Frequency')\n",
    "\n",
    "plt.suptitle('Word Frequency vs. Word Length', y=1.05)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313c324e",
   "metadata": {},
   "source": [
    "#### Count total number of word in each type of comment. comment without removing word and comment with wordlen > 3 and >4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8805f6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to store the word counts for each category\n",
    "word_counts = {}\n",
    "\n",
    "# Calculate word counts for each category for each column\n",
    "for column_name in column_names:\n",
    "    print(column_name)\n",
    "#     comment_length=data[column_name].str.len()\n",
    "    words = data[column_name].str.split().explode()\n",
    "    total_words = len(words)\n",
    "    words_lt_3 = words[words.str.len() < 3].count()\n",
    "    words_eq_3 = words[words.str.len()==3].count()\n",
    "    words_gte_4 = words[words.str.len() >= 4].count()\n",
    "    \n",
    "    word_counts[column_name] = {\n",
    "        'Total Words': total_words,\n",
    "        'Words < 3': words_lt_3,\n",
    "        'Words == 3': words_eq_3,\n",
    "        'Words >= 4': words_gte_4\n",
    "    }\n",
    "\n",
    "# Create a DataFrame from the word counts dictionary\n",
    "word_counts_df = pd.DataFrame(word_counts)\n",
    "\n",
    "# Display the word counts DataFrame\n",
    "print(word_counts_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22cbc60",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d677c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics = {}\n",
    "\n",
    "# Calculate statistics for each column\n",
    "for column_name in column_names:\n",
    "    word_lengths = data[column_name].str.split().explode().str.len()\n",
    "    statistics[column_name] = {\n",
    "        'Mean': word_lengths.mean(),\n",
    "        'Median': word_lengths.median(),\n",
    "        'Standard Deviation': word_lengths.std(),\n",
    "        'Minimum': word_lengths.min(),\n",
    "        'Maximum': word_lengths.max()\n",
    "    }\n",
    "\n",
    "# Create a DataFrame from the statistics dictionary\n",
    "statistics_df = pd.DataFrame(statistics)\n",
    "\n",
    "# Display the statistics DataFrame\n",
    "print(statistics_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45387a96",
   "metadata": {},
   "source": [
    "### <font color=\"blue\"> CHECK 3 AND 4 WORD LENGTH WORDS FREQUNECY</blue>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a607506",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_comments = [word_tokenize(comment) for comment in data[\"CorrectComment_Spell\"]]\n",
    "\n",
    "# Flatten the list of tokenized comments\n",
    "flat_tokens = [token for sublist in tokenized_comments for token in sublist if len(token)==3]\n",
    "\n",
    "# Create a frequency distribution\n",
    "freq_dist = FreqDist(flat_tokens)\n",
    "\n",
    "# Display the most common words and their frequencies\n",
    "print(\"Top 20 Most Common Words:\")\n",
    "print(freq_dist.most_common())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f876a3f",
   "metadata": {},
   "source": [
    "## <FONT COLOR=\"BLUE\">LEMMATIZATION USING NLTK</FONT>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0478a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"cleancomment2\"]=\"\"\n",
    "data[\"cleancomment3\"]=\"\"\n",
    "data[\"cleancomment4\"]=\"\"\n",
    "bows3=[]\n",
    "bows4=[]\n",
    "data['bow']=\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e4df6d",
   "metadata": {},
   "source": [
    "- cleancomment2,cleancomment3,cleancomment4 contain comment after the apply lemmatization.\n",
    "- It only contain word which match with POS ['VERB', 'NOUN', 'ADJ', 'ADV']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dda0d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.phrases import Phrases, ENGLISH_CONNECTOR_WORDS\n",
    "\n",
    "# Drop rows with missing values in 'comment_without_stopword' column\n",
    "data = data.dropna(subset=['CorrectComment_Spell'])\n",
    "\n",
    "# Convert any float values in 'comment_without_stopword' column to empty string\n",
    "data['CorrectComment_Spell'] = data['CorrectComment_Spell'].apply(lambda x: '' if isinstance(x, float) else x)\n",
    "\n",
    "# Load the spaCy English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Apply bigram and trigram\n",
    "bow = Phrases(data['CorrectComment_Spell'], min_count=1, threshold=1, connector_words=ENGLISH_CONNECTOR_WORDS)\n",
    "data['bow'] = data['CorrectComment_Spell'].apply(lambda x: list(bow[x.split()]))\n",
    "\n",
    "# Function to perform lemmatization with POS tagging and create new columns\n",
    "def lemmatize_with_pos_and_create_columns(text):\n",
    "    doc = nlp(text)\n",
    "    lemmatized_words = [token.lemma_ for token in doc if token.pos_ in ['VERB', 'NOUN', 'ADJ', 'ADV']]\n",
    "    cleancomment2 = \" \".join(lemmatized_words)\n",
    "    cleancomment3 = \" \".join([word for word in lemmatized_words if len(word) > 3])\n",
    "    cleancomment4 = \" \".join([word for word in lemmatized_words if len(word) > 4])\n",
    "    return cleancomment2, cleancomment3, cleancomment4\n",
    "\n",
    "# Apply the function to each row in the dataframe\n",
    "data[['cleancomment2', 'cleancomment3', 'cleancomment4']] = data[\"bow\"].apply(lambda x: pd.Series(lemmatize_with_pos_and_create_columns(' '.join(x))))\n",
    "\n",
    "# # Print the updated dataframe\n",
    "# print(data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545a364c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"cleancomment3\"][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0327dd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "# Initialize SpellChecker with English language\n",
    "spell = SpellChecker(language='en')\n",
    "\n",
    "# Define words to exclude from correction\n",
    "excluded_words = {'scanf', 'printf', 'getch', 'stdio', 'conio', 'clrscr', 'getchar',  \n",
    "                  'clrscr', 'void', 'int', 'datatype','runtime','specifier'}\n",
    "\n",
    "# Define manual corrections for specific words\n",
    "manual_corrections = {'flot': 'float', 'lot':'float','clrsccr': 'clrscr','flost':'float','\\x91clrscr':'clrscr',\n",
    "                      'specifi':'specifier','specific':'specifier'}\n",
    "\n",
    "# Function to spellcheck each comment\n",
    "def spellcheck_comment(comment):\n",
    "    if comment is None or comment == \"\":\n",
    "        return \"\", {}, 0\n",
    "\n",
    "    # Tokenize the comment\n",
    "    tokens = comment.split()\n",
    "\n",
    "    # Initialize dictionary to store misspelled words and their corrections\n",
    "    misspellings = {}\n",
    "\n",
    "    # Iterate through each word in the comment\n",
    "    for word in tokens:\n",
    "        # Check if the word is in the excluded list\n",
    "        if word.lower() in excluded_words:\n",
    "            continue  # Skip correction for excluded words\n",
    "        \n",
    "        # Check if the word is misspelled\n",
    "        corrected_word = spell.correction(word)\n",
    "        \n",
    "        # Check if the corrected word is one of the manual corrections\n",
    "        if corrected_word in manual_corrections:\n",
    "            corrected_word = manual_corrections[corrected_word]\n",
    "        \n",
    "        if corrected_word != word:\n",
    "            # Store the misspelled word and its corrected form in the dictionary\n",
    "            misspellings[word] = corrected_word\n",
    "\n",
    "    # Replace misspelled words with correct spelling in the comment\n",
    "    corrected_tokens = []\n",
    "    total_incorrect_words = 0\n",
    "    for word in tokens:\n",
    "        corrected_word = misspellings.get(word)\n",
    "        if corrected_word is not None:\n",
    "            corrected_tokens.append(corrected_word)\n",
    "            total_incorrect_words += 1\n",
    "        else:\n",
    "            corrected_tokens.append(word)\n",
    "    \n",
    "    corrected_comment = ' '.join(corrected_tokens)\n",
    "\n",
    "    return corrected_comment, misspellings, total_incorrect_words\n",
    "\n",
    "# Apply spellchecking to each comment in the dataframe for cleancomment2, cleancomment3, and cleancomment4\n",
    "data['CorrectComment2_Spell'], data['misspellings2'], data['total_incorrect_words2'] = zip(*data['cleancomment2'].apply(spellcheck_comment))\n",
    "data['CorrectComment3_Spell'], data['misspellings3'], data['total_incorrect_words3'] = zip(*data['cleancomment3'].apply(spellcheck_comment))\n",
    "data['CorrectComment4_Spell'], data['misspellings4'], data['total_incorrect_words4'] = zip(*data['cleancomment4'].apply(spellcheck_comment))\n",
    "\n",
    "# Create a dictionary of incorrect spellings against correct spellings for cleancomment2\n",
    "incorrect_correct_dict2 = {}\n",
    "for misspelling_dict in data['misspellings2']:\n",
    "    for misspelled_word, corrected_word in misspelling_dict.items():\n",
    "        if misspelled_word not in incorrect_correct_dict2:\n",
    "            incorrect_correct_dict2[misspelled_word] = corrected_word\n",
    "\n",
    "# Create a dictionary of incorrect spellings against correct spellings for cleancomment3\n",
    "incorrect_correct_dict3 = {}\n",
    "for misspelling_dict in data['misspellings3']:\n",
    "    for misspelled_word, corrected_word in misspelling_dict.items():\n",
    "        if misspelled_word not in incorrect_correct_dict3:\n",
    "            incorrect_correct_dict3[misspelled_word] = corrected_word\n",
    "\n",
    "# Create a dictionary of incorrect spellings against correct spellings for cleancomment4\n",
    "incorrect_correct_dict4 = {}\n",
    "for misspelling_dict in data['misspellings4']:\n",
    "    for misspelled_word, corrected_word in misspelling_dict.items():\n",
    "        if misspelled_word not in incorrect_correct_dict4:\n",
    "            incorrect_correct_dict4[misspelled_word] = corrected_word\n",
    "\n",
    "# Create new dataframes with required columns\n",
    "new_data2 = data[['ID', 'Name', 'Comment1', 'cleancomment2', 'misspellings2', 'CorrectComment2_Spell', 'total_incorrect_words2']]\n",
    "new_data3 = data[['ID', 'Name', 'Comment1', 'cleancomment3', 'misspellings3', 'CorrectComment3_Spell', 'total_incorrect_words3']]\n",
    "new_data4 = data[['ID', 'Name', 'Comment1', 'cleancomment4', 'misspellings4', 'CorrectComment4_Spell', 'total_incorrect_words4']]\n",
    "\n",
    "# Display the corrected comments and misspelled words for cleancomment2\n",
    "print(new_data2)\n",
    "\n",
    "# Display the corrected comments and misspelled words for cleancomment3\n",
    "print(new_data3)\n",
    "\n",
    "# Display the corrected comments and misspelled words for cleancomment4\n",
    "print(new_data4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1fcd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%%capture cap\n",
    "\n",
    "def print_pos_tag(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    for token in doc:\n",
    "         if token.pos_ in [\"VERB\", \"ADJ\", \"ADV\", \"NOUN\"]:\n",
    "            print(token.text, token.pos_)\n",
    "\n",
    "# Apply the function to each row in the \"cleancomment3\" column\n",
    "for sentence in data[\"cleancomment3\"]:\n",
    "#       for w in sentence:\n",
    "         print_pos_tag(sentence)\n",
    "          #print(sentence)\n",
    "\n",
    "# spellcheck2(data,'comment_without_stopword')\n",
    "# spellcheck2(data,'CleanComment3')\n",
    "# spellcheck2(data,'CleanComment4')\n",
    "\n",
    "with open('c:\\\\Research\\\\POS1.txt', 'w',encoding=\"utf-8\") as file:\n",
    "    file.write(cap.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15719d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_pos_tag(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    for token in doc:\n",
    "         if token.pos_ in [\"VERB\", \"ADJ\", \"ADV\", \"NOUN\"]:\n",
    "            print(token.text, token.pos_)\n",
    "\n",
    "print_pos_tag(lem_teacher_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f30b436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_comments = [word_tokenize(comment) for comment in data[\"cleancomment3\"]]\n",
    "\n",
    "# # Flatten the list of tokenized comments\n",
    "# flat_tokens = [token for sublist in tokenized_comments for token in sublist if len(token)>3]\n",
    "\n",
    "# # Create a frequency distribution\n",
    "# freq_dist = FreqDist(flat_tokens)\n",
    "\n",
    "# # Display the most common words and their frequencies\n",
    "# print(\"Top 20 Most Common Words:\")\n",
    "# print(freq_dist.most_common())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f333f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "newdata=data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b772737c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find indices of rows with empty strings or lists in the \"cleancomment2\" column\n",
    "empty_indices = newdata[newdata['cleancomment3'].apply(lambda x: len(x) == 0)].index\n",
    "\n",
    "# Print the indices of rows with empty strings or lists\n",
    "print(\"Indices of rows with empty strings or lists:\",(empty_indices))\n",
    "print(\"Length of newdata dataframe is:\",len(newdata))\n",
    "print(\"Total row is :\",len(newdata)-len(empty_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6dfa039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with empty lists in the \"cleancomment2\" column\n",
    "newdata = newdata[newdata['cleancomment3'].apply(lambda x: len(x) > 0)]\n",
    "\n",
    "# Reset index\n",
    "newdata.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(len(newdata))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b36575",
   "metadata": {},
   "source": [
    "### <FONT COLOR=\"BLUE\">ANALYSE WORD FREQUENCY BEFORE AND AFTER LEMMATIZATION</FONT>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2584e62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"lem\"]=\"\"\n",
    "freq_dist=\" \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd035002",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_comments = [word_tokenize(comment) for comment in newdata[\"CorrectComment3_Spell\"]]\n",
    "\n",
    "# Flatten the list of tokenized comments\n",
    "flat_tokens = [token for sublist in tokenized_comments for token in sublist ] #if len(token)>4]\n",
    "\n",
    "# Create a frequency distribution\n",
    "freq_dist = FreqDist(flat_tokens)\n",
    "\n",
    "# Display the most common words and their frequencies\n",
    "print(\"Top 20 Most Common Words:\")\n",
    "print(freq_dist.most_common(20))\n",
    "# data[\"lem3\"] = data[\"cleancomment3\"].apply(lambda x: \"\".join(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d819bf1",
   "metadata": {},
   "source": [
    "### <FONT COLOR=\"BLUE\">ANALYSE WORD FREQUENCY BEFORE AND AFTER LEMMATIZATION</FONT>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf845ef",
   "metadata": {},
   "source": [
    "### Find Frequncy Distribution After lemmmatization cleancomment2 (Contain all length word) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68fc36c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"lem\"]=\"\"\n",
    "freq_dist=\" \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4248ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_comments = [word_tokenize(comment) for comment in newdata[\"CorrectComment2_Spell\"]]\n",
    "\n",
    "# Flatten the list of tokenized comments\n",
    "flat_tokens = [token for sublist in tokenized_comments for token in sublist ] #if len(token)>4]\n",
    "\n",
    "# Create a frequency distribution\n",
    "freq_dist = FreqDist(flat_tokens)\n",
    "\n",
    "# Display the most common words and their frequencies\n",
    "print(\"Top 20 Most Common Words:\")\n",
    "print(freq_dist.most_common(20))\n",
    "# data[\"lem3\"] = data[\"cleancomment3\"].apply(lambda x: \"\".join(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff1653c",
   "metadata": {},
   "source": [
    "### <FONT COLOR=\"BLUE\"> Word Frequency Distribution Plot After Lemmatization </FONT>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbc806d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.tokenize import word_tokenize\n",
    "# from nltk.probability import FreqDist\n",
    "# from matplotlib import pyplot as plt\n",
    "\n",
    "def plot_top_words(ax, column, title):\n",
    "    # Tokenize comments in the specified column\n",
    "    tokenized_comments = [word_tokenize(comment) for comment in column]\n",
    "    \n",
    "    # Flatten the list of tokenized comments\n",
    "    flat_tokens = [token for sublist in tokenized_comments for token in sublist]\n",
    "    \n",
    "    # Create a frequency distribution\n",
    "    freq_dist = FreqDist(flat_tokens)\n",
    "    \n",
    "    # Get the top 20 most common words\n",
    "    top_words = freq_dist.most_common(20)\n",
    "    x, y = zip(*top_words)\n",
    "    \n",
    "    # Plot the top 20 most common words\n",
    "    ax.bar(x, y)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('Words')\n",
    "    ax.set_ylabel('Frequency')\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Plot for cleancomment2\n",
    "# plot_top_words(axes[0], newdata[\"cleancomment2\"], 'Top 20 Most Common Words in cleancomment2')\n",
    "\n",
    "# Plot for cleancomment3\n",
    "plot_top_words(axes[0], newdata[\"CorrectComment3_Spell\"], 'Top 20 Most Common Words in cleancomment3')\n",
    "\n",
    "# Plot for cleancomment4\n",
    "plot_top_words(axes[1], newdata[\"CorrectComment4_Spell\"], 'Top 20 Most Common Words in cleancomment4')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51893d64",
   "metadata": {},
   "source": [
    "### WORDFREQUENCY PLOT AFTER LEMMATIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf67fa45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to count word frequency for a given column\n",
    "def count_word_frequency(column):\n",
    "    word_length_counts = column.str.split().explode().str.len().value_counts().sort_index()\n",
    "    return word_length_counts\n",
    "# Plot word frequency against word length for each column in one row\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5), sharey=True)\n",
    "\n",
    "# Column names\n",
    "column_names = ['CleanComment', 'CleanComment3', 'CleanComment4']\n",
    "    \n",
    "for i, column_name in enumerate(column_names):\n",
    "        word_length_counts = count_word_frequency(newdata[column_name])\n",
    "        axes[i].bar(word_length_counts.index, word_length_counts.values, color='skyblue')\n",
    "        axes[i].set_title(column_name)\n",
    "        axes[i].set_xlabel('Word Length')\n",
    "        axes[i].set_ylabel('Frequency')\n",
    "\n",
    "plt.suptitle('Word Frequency vs. Word Length Before Lemmatization', y=1.05)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# Plot word frequency against word length for each column in one row\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5), sharey=True)\n",
    "\n",
    "# Column names\n",
    "column_names = ['CorrectComment2_Spell', 'CorrectComment3_Spell', 'CorrectComment4_Spell']\n",
    "    \n",
    "for i, column_name in enumerate(column_names):\n",
    "        word_length_counts = count_word_frequency(newdata[column_name])\n",
    "        axes[i].bar(word_length_counts.index, word_length_counts.values, color='skyblue')\n",
    "        axes[i].set_title(column_name)\n",
    "        axes[i].set_xlabel('Word Length')\n",
    "        axes[i].set_ylabel('Frequency')\n",
    "\n",
    "plt.suptitle('Word Frequency vs. Word Length After Lemmatization', y=1.05)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2871d6bc",
   "metadata": {},
   "source": [
    "### Find Statistic of Before and After Lemmatization of All Word, Word Len > 3 and Word Len > 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf51872",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to store the word counts for each category\n",
    "word_counts = {}\n",
    "\n",
    "# Calculate word counts for each category for each column\n",
    "for column_name in column_names:\n",
    "    words = newdata[column_name].str.split().explode()\n",
    "    total_words = len(words)\n",
    "    words_lt_3 = words[words.str.len() < 3].count()\n",
    "    words_eq_3 = words[words.str.len() == 3].count()\n",
    "    words_gte_4 = words[words.str.len() >= 4].count()\n",
    "    \n",
    "    word_counts[column_name] = {\n",
    "        'Total Words': total_words,\n",
    "        'Words < 3': words_lt_3,\n",
    "        'Words = 3': words_eq_3,\n",
    "        'Words >= 4': words_gte_4\n",
    "    }\n",
    "\n",
    "# Create a DataFrame from the word counts dictionary\n",
    "newword_counts_df = pd.DataFrame(word_counts)\n",
    "\n",
    "# Display the word counts DataFrame\n",
    "print(\"\\t\\t\\tWord Count Statistics After Lemmatization\\n=====================================================================================\")\n",
    "print(newword_counts_df)\n",
    "print(\"\\n\\n\\t\\t\\tWord Count Statistics before Lemmatization\\n=====================================================================================\")\n",
    "print(word_counts_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80622e42",
   "metadata": {},
   "source": [
    "### <FONT COLOR=\"BLUE\"> PRINT WORD CLOUD </FONT>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff51a19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cead26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordfreq(data,colname):\n",
    "    \n",
    "    tokenized_comments = [word_tokenize(comment) for comment in newdata[colname]]\n",
    "\n",
    "    # Flatten the list of tokenized comments\n",
    "    flat_tokens = [token for sublist in tokenized_comments for token in sublist ] #if len(token)>4]\n",
    "\n",
    "    # Create a frequency distribution\n",
    "    freq_dist = FreqDist(flat_tokens)\n",
    "    return freq_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fb76c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from wordcloud import WordCloud\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "col=['CorrectComment2_Spell','CorrectComment3_Spell','CorrectComment4_Spell']\n",
    "# freqdist(newdata,l)\n",
    "# Generate word clouds\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "for i, (column, ax) in enumerate(zip(['CorrectComment2_Spell', 'CorrectComment3_Spell', 'CorrectComment4_Spell'], axes)):\n",
    "#     print(f\"{i}--->{column}--->{ax}\")\n",
    "    wordcloud = WordCloud(width=300, height=300, background_color='white', max_font_size=100, colormap='viridis').generate_from_frequencies(wordfreq(newdata,column))\n",
    "    ax.imshow(wordcloud, interpolation='bilinear')\n",
    "    ax.set_title(f'Word Cloud - {column}')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b1c093",
   "metadata": {},
   "source": [
    "### <FONT COLOR=\"BLUE\"> COMMENT CATEGORY </FONT>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7cb0d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define error and solution keywords\n",
    "error_keywords = [\"error\", \"syntax\", \"logical\", \"runtime\", \"exception\", \"bug\", \"issue\", \"mistake\", \"flaw\", \"incorrect\", \"fault\", \"problem\", \"debug\", \"crash\"]\n",
    "solution_keywords = [\"solution\", \"fix\", \"resolve\", \"correct\", \"handle\", \"patch\", \"repair\", \"address\", \"amend\", \"rectify\", \"debugging\", \"workaround\"]\n",
    "# Function to categorize comments into error only, solution only, or both\n",
    "def categorize_comment(comment):\n",
    "    contains_error = any(keyword in comment.lower() for keyword in error_keywords)\n",
    "    contains_solution = any(keyword in comment.lower() for keyword in solution_keywords)\n",
    "    \n",
    "    if contains_error and not contains_solution:\n",
    "        return \"Error Only\"\n",
    "    elif contains_solution and not contains_error:\n",
    "        return \"Solution Only\"\n",
    "    elif contains_error and contains_solution:\n",
    "        return \"Both Error and Solution\"\n",
    "    else:\n",
    "        return \"None\"\n",
    "\n",
    "# Apply the categorization function to each comment in the DataFrame column \"cleancomment2\"\n",
    "newdata['Comment_Category'] = newdata['CorrectComment3_Spell'].apply(categorize_comment)\n",
    "\n",
    "# Display the DataFrame with the new column\n",
    "print(newdata[['ID','CorrectComment3_Spell', 'Comment_Category']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74ebaac",
   "metadata": {},
   "source": [
    "### <FONT COLOR=\"BLUE\">FIND MATCHING ANSWER KEYWORD</BLUE>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3746602b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find matching keywords between student's comment and teacher answer keywords\n",
    "d=0\n",
    "def find_matching_keywords(comment, keywords):\n",
    "#     matching_keywords = [keyword for keyword in keywords if keyword in comment.split()]\n",
    "    matching_keywords=[]\n",
    "    for keyword in keywords:\n",
    "        if keyword in comment.split():\n",
    "            matching_keywords.append(keyword)\n",
    "    global d\n",
    "    d=d+1\n",
    "    if matching_keywords:\n",
    "        print(f\"{d}-->{comment}-->{','.join(matching_keywords)}\")\n",
    "        return ','.join(matching_keywords)\n",
    "    else:\n",
    "        return \"No Matching\"\n",
    "\n",
    "# Apply the function to each row in the dataframe\n",
    "\n",
    "teacher_keywords = ['variable', 'declaration', 'integer', 'int','undeclare', 'declare','datatype']\n",
    "newdata['Match_with_Answer_keywords3'] = newdata['CorrectComment3_Spell'].apply(lambda x: find_matching_keywords(x, teacher_keywords))\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(newdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188b6839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find matching keywords between student's comment and teacher answer keywords\n",
    "d=0\n",
    "def find_matching_keywords(comment, keywords):\n",
    "#     matching_keywords = [keyword for keyword in keywords if keyword in comment.split()]\n",
    "    matching_keywords=[]\n",
    "    for keyword in keywords:\n",
    "        if keyword in comment.split():\n",
    "            matching_keywords.append(keyword)\n",
    "    global d\n",
    "    d=d+1\n",
    "    if matching_keywords:\n",
    "        print(f\"{d}-->{comment}-->{','.join(matching_keywords)}\")\n",
    "        return ','.join(matching_keywords)\n",
    "    else:\n",
    "        return \"No Matching\"\n",
    "\n",
    "# Apply the function to each row in the dataframe\n",
    "\n",
    "teacher_keywords = teacher_keywords = ['variable', 'declaration', 'integer', 'undeclare', 'declare','datatype']\n",
    "newdata['Match_with_Answer_keywords2'] = newdata['CorrectComment2_Spell'].apply(lambda x: find_matching_keywords(x, teacher_keywords))\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(newdata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25cd709e",
   "metadata": {},
   "source": [
    "### <FONT COLOR=\"BLUE\"> STORE ERROR AND SOLUTION MATCHING KEYWORD </FONT>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b9f3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample student comments\n",
    "student_comments = newdata[\"CorrectComment2_Spell\"]\n",
    "\n",
    "# Define error and solution keywords\n",
    "error_keywords = [\"error\", \"wrong\",\"syntax\", \"logical\", \"runtime\", \"exception\", \"bug\", \"issue\", \"mistake\", \"flaw\", \"incorrect\", \"fault\", \"problem\", \"debug\", \"crash\"]\n",
    "solution_keywords = [\"solution\", \"fix\", \"resolve\", \"correct\", \"corrected\",\"handle\", \"patch\", \"repair\", \"address\", \"amend\", \"rectify\", \"debugging\", \"workaround\"]\n",
    "\n",
    "# Initialize lists to store comments categorized as error only, solution only, and both error and solution\n",
    "error_only_comments = []\n",
    "solution_only_comments = []\n",
    "both_error_solution_comments = []\n",
    "\n",
    "# Initialize lists to store matching keywords for each category\n",
    "error_keywords_matched = []\n",
    "solution_keywords_matched = []\n",
    "both_keywords_matched = []\n",
    "\n",
    "# Iterate through each student comment\n",
    "for comment in student_comments:\n",
    "    contains_error = any(keyword in comment.lower() for keyword in error_keywords)\n",
    "    contains_solution = any(keyword in comment.lower() for keyword in solution_keywords)\n",
    "    \n",
    "    error_matched_keywords = [keyword for keyword in error_keywords if keyword in comment.lower()]\n",
    "    solution_matched_keywords = [keyword for keyword in solution_keywords if keyword in comment.lower()]\n",
    "    \n",
    "    if contains_error and not contains_solution:\n",
    "        error_only_comments.append(comment)\n",
    "        error_keywords_matched.append(','.join(error_matched_keywords))\n",
    "        solution_keywords_matched.append('')\n",
    "        both_keywords_matched.append('')\n",
    "    elif contains_solution and not contains_error:\n",
    "        solution_only_comments.append(comment)\n",
    "        solution_keywords_matched.append(','.join(solution_matched_keywords))\n",
    "        error_keywords_matched.append('')\n",
    "        both_keywords_matched.append('')\n",
    "    elif contains_error and contains_solution:\n",
    "        both_error_solution_comments.append(comment)\n",
    "        both_keywords_matched.append(','.join(error_matched_keywords + solution_matched_keywords))\n",
    "        error_keywords_matched.append(','.join(error_matched_keywords))\n",
    "        solution_keywords_matched.append(','.join(solution_matched_keywords))\n",
    "    else:\n",
    "        error_keywords_matched.append('')\n",
    "        solution_keywords_matched.append('')\n",
    "        both_keywords_matched.append('')\n",
    "        \n",
    "print(len(both_keywords_matched))\n",
    "print(len(solution_keywords_matched))\n",
    "print(len(error_keywords_matched))\n",
    "# Add the matched keyword columns to the DataFrame\n",
    "newdata['Error_Matched_Keywords2'] = error_keywords_matched\n",
    "newdata['Solution_Matched_Keywords2'] = solution_keywords_matched\n",
    "newdata['Both_Matched_Keywords2'] = both_keywords_matched\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404c8664",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample student comments\n",
    "student_comments = newdata[\"CorrectComment3_Spell\"]\n",
    "\n",
    "# Define error and solution keywords\n",
    "error_keywords = [\"error\", \"wrong\",\"syntax\", \"logical\", \"runtime\", \"exception\", \"bug\", \"issue\", \"mistake\", \"flaw\", \"incorrect\", \"fault\", \"problem\", \"debug\", \"crash\"]\n",
    "solution_keywords = [\"solution\", \"fix\", \"resolve\", \"correct\", \"corrected\",\"handle\", \"patch\", \"repair\", \"address\", \"amend\", \"rectify\", \"debugging\", \"workaround\"]\n",
    "\n",
    "# Initialize lists to store comments categorized as error only, solution only, and both error and solution\n",
    "error_only_comments = []\n",
    "solution_only_comments = []\n",
    "both_error_solution_comments = []\n",
    "\n",
    "# Initialize lists to store matching keywords for each category\n",
    "error_keywords_matched = []\n",
    "solution_keywords_matched = []\n",
    "both_keywords_matched = []\n",
    "\n",
    "# Iterate through each student comment\n",
    "for comment in student_comments:\n",
    "    contains_error = any(keyword in comment.lower() for keyword in error_keywords)\n",
    "    contains_solution = any(keyword in comment.lower() for keyword in solution_keywords)\n",
    "    \n",
    "    error_matched_keywords = [keyword for keyword in error_keywords if keyword in comment.lower()]\n",
    "    solution_matched_keywords = [keyword for keyword in solution_keywords if keyword in comment.lower()]\n",
    "    \n",
    "    if contains_error and not contains_solution:\n",
    "        error_only_comments.append(comment)\n",
    "        error_keywords_matched.append(','.join(error_matched_keywords))\n",
    "        solution_keywords_matched.append('')\n",
    "        both_keywords_matched.append('')\n",
    "    elif contains_solution and not contains_error:\n",
    "        solution_only_comments.append(comment)\n",
    "        solution_keywords_matched.append(','.join(solution_matched_keywords))\n",
    "        error_keywords_matched.append('')\n",
    "        both_keywords_matched.append('')\n",
    "    elif contains_error and contains_solution:\n",
    "        both_error_solution_comments.append(comment)\n",
    "        both_keywords_matched.append(','.join(error_matched_keywords + solution_matched_keywords))\n",
    "        error_keywords_matched.append(','.join(error_matched_keywords))\n",
    "        solution_keywords_matched.append(','.join(solution_matched_keywords))\n",
    "    else:\n",
    "        error_keywords_matched.append('')\n",
    "        solution_keywords_matched.append('')\n",
    "        both_keywords_matched.append('')\n",
    "        \n",
    "print(len(both_keywords_matched))\n",
    "print(len(solution_keywords_matched))\n",
    "print(len(error_keywords_matched))\n",
    "# Add the matched keyword columns to the DataFrame\n",
    "newdata['Error_Matched_Keywords3'] = error_keywords_matched\n",
    "newdata['Solution_Matched_Keywords3'] = solution_keywords_matched\n",
    "newdata['Both_Matched_Keywords3'] = both_keywords_matched\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c550c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "newdata.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ec64d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "newdata[newdata[\"Comment_Category\"]==\"Both Error and Solution\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa6baea",
   "metadata": {},
   "source": [
    "### <FONT COLOR=\"BLUE\">GENDER-WISE COUNT FOR COMMENT CATEGORY MEANS COMMENT IS ERROR, SOLUTION OR BOTH.</FONT>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a267e158",
   "metadata": {},
   "outputs": [],
   "source": [
    "newdata.groupby(['Gender'])[\"Comment_Category\"].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d42210c",
   "metadata": {},
   "source": [
    "### <FONT COLOR=\"BLUE\">FIND COSSINE SIMILARITY FOR ALL COMMENT</BLUE>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55eb8adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Teacher's answer and keywords\n",
    "# teacher_answer = \"error related to variable declartion. Forget to declare datatype of variable during declaration. so it generate syntax error. Solution : we have to declare variable integer because in printf statement it use %d as format specifier.\"\n",
    "# teacher_keywords = ['variable', 'declaration', 'integer', 'undeclare', 'declare','datatype']\n",
    "\n",
    "# Sample student comments (assuming they are stored in a DataFrame)\n",
    "student_comments = newdata[\"CorrectComment3_Spell\"]\n",
    "\n",
    "# Initialize CountVectorizer with teacher's keywords\n",
    "vectorizer = CountVectorizer(vocabulary=teacher_keywords)\n",
    "\n",
    "# Transform teacher's answer and student comments separately\n",
    "teacher_vector = vectorizer.fit_transform([teacher_answer])\n",
    "student_vector = vectorizer.transform(student_comments)\n",
    "\n",
    "# Calculate cosine similarity between teacher's answer and student comments\n",
    "similarity_scores = cosine_similarity(teacher_vector, student_vector)\n",
    "\n",
    "# Add cosine similarity scores to the DataFrame\n",
    "newdata['Cosine_Similarity_Sent'] = similarity_scores.flatten()  # Flatten to make it 1D\n",
    "\n",
    "# Display the DataFrame with cosine similarity scores\n",
    "print(newdata[['ID', 'cleancomment3', 'Cosine_Similarity_Sent']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8870abfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install spicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86eee504",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.similarity import cosine_similarity\n",
    "\n",
    "# Sample vectors (replace with your actual data)\n",
    "vec1 = [1, 2, 3]\n",
    "vec2 = [4, 5, 6]\n",
    "\n",
    "similarity = cosine_similarity(vec1, vec2)\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e603d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Teacher's answer (already lemmatized)\n",
    "teacher_answer = lem_teacher_answer\n",
    "\n",
    "# Student comments from DataFrame\n",
    "student_comments = newdata[\"CorrectComment3_Spell\"].tolist()\n",
    "\n",
    "# Preprocess student comments (optional): lowercase, remove punctuation, etc.\n",
    "# ... (Add your preprocessing steps here if needed)\n",
    "\n",
    "# Create TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Transform text into TF-IDF vectors\n",
    "teacher_features = vectorizer.fit_transform([teacher_answer])\n",
    "student_features = vectorizer.transform(student_comments)  # Transform all comments at once\n",
    "\n",
    "# Calculate cosine similarity for each student comment\n",
    "from scipy.spatial.similarity import cosine_similarity\n",
    "\n",
    "similarities = cosine_similarity(teacher_features, student_features)[0]\n",
    "\n",
    "# Loop through student comments and print similarity scores\n",
    "for i, comment in enumerate(student_comments):\n",
    "  print(f\"Cosine Similarity between Teacher Answer and Comment {i+1}:\", similarities[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39fcc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Teacher's keywords\n",
    "teacher_keywords = ['variable', 'declaration', 'integer', 'undeclare', 'declare','datatype']\n",
    "\n",
    "# Sample student comments (assuming they are stored in a DataFrame)\n",
    "student_comments = newdata[\"CorrectComment3_Spell\"]\n",
    "\n",
    "# Initialize CountVectorizer with teacher's keywords\n",
    "vectorizer = CountVectorizer(vocabulary=teacher_keywords)\n",
    "\n",
    "# Transform student comments\n",
    "student_vector = vectorizer.transform(student_comments)\n",
    "\n",
    "# Calculate cosine similarity between teacher's keywords and student comments\n",
    "teacher_vector = vectorizer.transform([' '.join(teacher_keywords)])  # Combine teacher's keywords into a single document\n",
    "similarity_scores = cosine_similarity(teacher_vector, student_vector)\n",
    "\n",
    "# Add cosine similarity scores to the DataFrame\n",
    "newdata['Cosine_Similarity_Keywords'] = similarity_scores.flatten()  # Flatten to make it 1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5361d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "newdata.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838ca609",
   "metadata": {},
   "outputs": [],
   "source": [
    " newdata[[\"ID\",\"Cosine_Similarity_Sent\",\"Cosine_Similarity_Keywords\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4f9694",
   "metadata": {},
   "source": [
    "### <font color=\"Blue\">EVALUATION AND ASSIGN MARKS</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b850b908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# newdata2[\"Total_Answer_Keyword\"]=newdata2['Match_with_Answer_keywords'].apply(lambda x:len(x.split(\",\")))\n",
    "\n",
    "# Apply the function to create the \"Total_Answer_Keyword\" column\n",
    "newdata[\"Total_Answer_Keyword2\"] = newdata['Match_with_Answer_keywords2'].apply(lambda x: 0 if x == \"No Matching\" else len(x.split(\",\")))\n",
    "\n",
    "# Display the updated DataFrame with the new column\n",
    "# print(newdata2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e2fbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# newdata2[\"Total_Answer_Keyword\"]=newdata2['Match_with_Answer_keywords'].apply(lambda x:len(x.split(\",\")))\n",
    "\n",
    "# Apply the function to create the \"Total_Answer_Keyword\" column\n",
    "newdata[\"Total_Answer_Keyword3\"] = newdata['Match_with_Answer_keywords3'].apply(lambda x: 0 if x == \"No Matching\" else len(x.split(\",\")))\n",
    "\n",
    "# Display the updated DataFrame with the new column\n",
    "# print(newdata2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad60dc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to create the \"Cosine_Value\" column\n",
    "newdata[\"Cosine_Value_Keywords\"] = newdata['Cosine_Similarity_Keywords'].apply(lambda x: \"High\" if x > 0.70 else \"Low\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d67f6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the total marks\n",
    "total_marks = 50\n",
    "\n",
    "# Define the criteria and corresponding marks allocation\n",
    "marks_criteria = {\n",
    "    'Cosine_Value_Keywords': {'High': 15, 'Low': 10},\n",
    "    'CommentType3': {'Detail': 10, 'Basic': 5},\n",
    "    'Comment_Category': {'Both Error and Solution': 10, 'Solution Only': 5, 'Error Only': 5, 'None': 2},\n",
    "    'Total_Answer_Keyword2': {'>4': 15, '=4': 15, '<4': 10, '=0': 1}\n",
    "}\n",
    "teacher_keywords = ['variable', 'declaration', 'integer', 'undeclare', 'declare','datatype']\n",
    "lenanskeyword= len(teacher_keywords)\n",
    "\n",
    "# Function to assign marks based on the criteria\n",
    "def assign_marks(row):\n",
    "    marks = 0\n",
    "    assignmarks=[]\n",
    "    for criterion, allocation in marks_criteria.items():\n",
    "        value = row[criterion]\n",
    "#         print(f\"{value}-->{allocation}\")\n",
    "        \n",
    "        if value in allocation:\n",
    "            marks += allocation[value]\n",
    "            assignmarks.append(allocation[value])\n",
    "        elif value >= (lenanskeyword)*0.60:\n",
    "#             print(f\"{value}-->{(lenanskeyword)*0.60}\")\n",
    "            marks += 15\n",
    "            assignmarks.append(value)\n",
    "        elif value <= (lenanskeyword)*0.60:\n",
    "#             print(f\"{value}-->{(lenanskeyword)*0.60}\")\n",
    "            marks += 10\n",
    "            assignmarks.append(value)\n",
    "        elif value == 0:\n",
    "#             print(f\"{value}-->{(lenanskeyword)*0.60}\")\n",
    "            assignmarks.append(value)\n",
    "            marks += 0\n",
    "        else:\n",
    "             marks += 0  # If criterion not found, assign 0 marks\n",
    "#         print(assignmarks)\n",
    "    return marks\n",
    "# Apply the function to the DataFrame to calculate the marks for each comment\n",
    "newdata['Mark_With_4Rule'] = newdata.apply(assign_marks, axis=1)\n",
    "\n",
    "# Display the updated DataFrame with marks\n",
    "print(newdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c2c6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "newdata.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c7775f",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_keywords = ['variable', 'declaration', 'integer','int', 'undeclare', 'declare', 'datatype']\n",
    "total_answer_keywrods=len(teacher_keywords)\n",
    "newdata['Total_Answer_keywords_per']=newdata['Total_Answer_Keyword3'].apply(lambda x:((x*100)/total_answer_keywrods))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55df5e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "newdata[['ID','Total_Answer_Keyword3','Total_Answer_keywords_per']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17ba384",
   "metadata": {},
   "source": [
    "### <FONT COLOR='BLUE'> WRITE DATA TO FILE TO ASSIGN MANUAL MARK </FONT>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b0938d",
   "metadata": {},
   "outputs": [],
   "source": [
    "newdata.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d6216e",
   "metadata": {},
   "outputs": [],
   "source": [
    "manual=pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b1e65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "manual=newdata[['ID','Name','Gender', 'Email Address', 'Timestatmp',\n",
    "       'Comment1', 'CleanComment', 'comment_without_stopword',\n",
    "       'Comment_Length', 'comment_after_bitrigram', 'CorrectComment_Spell',\n",
    "       'misspellings', 'total_incorrect_words', 'CleanComment3',\n",
    "       'CleanComment4', 'TotalWordgte3', 'TotalWordgte4',\n",
    "       'CleanComment3_Length', 'CommentType3','cleancomment3','Match_with_Answer_keywords3','CorrectComment3_Spell','CommentType3','Comment_Category','Total_Answer_keywords_per','Total_Answer_Keyword3','Cosine_Similarity_Sent','Cosine_Similarity_Keywords']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a64d8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv_file_path = 'G:\\\\GitHub Classroom Assistant\\\\Topic Modelling Dataset\\\\TMC\\\\Formanualmarkass-1.csv'\n",
    "csv_file_path = 'G:\\\\GitHub Classroom Assistant\\\\Topic Modelling Dataset\\\\TMC\\\\Formanualmarkass-12.csv'\n",
    "manual.to_csv(csv_file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17204a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "file_path = \"G:\\\\GitHub Classroom Assistant\\\\Topic Modelling Dataset\\\\TMC\\\\Formanualmarkass-1.csv\"\n",
    "mdata = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f59013a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mdata.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e864fa6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "# from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "# import joblib\n",
    "\n",
    "# # Load the training data\n",
    "# file_path = r'G:\\GitHub Classroom Assistant\\Topic Modelling Dataset\\TMC\\Formanualmarkass-1.csv'\n",
    "# data = pd.read_csv(file_path)\n",
    "\n",
    "# # Preprocess the data (if necessary)\n",
    "# # This may include handling missing values, encoding categorical variables, and scaling numerical features\n",
    "\n",
    "# encoder_3rule = OneHotEncoder(drop='first', sparse=False)\n",
    "# X_encoded_3rule = encoder_3rule.fit_transform(newdata[['CommentType3', 'Comment_Category']].values)\n",
    "# X_processed_3rule = np.concatenate([X_encoded_3rule, newdata[['Total_Answer_keywords_per']].values], axis=1)\n",
    "# y_3rule = data['Manual_Mark']\n",
    "\n",
    "# # Step 4: Split the data into training and testing sets for the 3-rule method\n",
    "# X_train_3rule, X_test_3rule, y_train_3rule, y_test_3rule = train_test_split(X_processed_3rule, y_3rule, test_size=0.2, random_state=42)\n",
    "# # Split the data into training and testing sets\n",
    "# # X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Choose a machine learning algorithm and create an instance of the model\n",
    "# model = RandomForestRegressor()\n",
    "\n",
    "# # Train the model on the training data\n",
    "# model.fit(X_train_3rule, y_train_3rule)\n",
    "\n",
    "# # Evaluate the performance of the trained model\n",
    "# y_pred = model.predict(X_test_3rule)\n",
    "# mse = mean_squared_error(y_test_3rule, y_pred)\n",
    "# mae = mean_absolute_error(y_test_3rule, y_pred)\n",
    "# r2 = r2_score(y_test_3rule, y_pred)\n",
    "\n",
    "# print(\"Mean Squared Error:\", mse)\n",
    "# print(\"Mean Absolute Error:\", mae)\n",
    "# print(\"R^2 Score:\", r2)\n",
    "\n",
    "# # Optionally, save the trained model for future use\n",
    "# joblib.dump(model, 'trained_model.pkl')\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"G:\\\\GitHub Classroom Assistant\\\\Topic Modelling Dataset\\\\TMC\\\\Formanualmarkass-12.csv\"\n",
    "mdata = pd.read_csv(file_path)\n",
    "\n",
    "# Replace 'None' with 0 in the comment_category column\n",
    "mdata['Comment_Category'].replace('None', 0, inplace=True)\n",
    "\n",
    "# Select features and target variable\n",
    "X = mdata[['CommentType3', 'Comment_Category', 'Total_Answer_keywords_per']]\n",
    "y = mdata['Manual_Mark']\n",
    "\n",
    "# Convert categorical features to numerical using one-hot encoding\n",
    "X_encoded = pd.get_dummies(X, drop_first=True)\n",
    "# print(mdata.Comment_Category)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Random Forest model\n",
    "rf_model = RandomForestRegressor()\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict marks for test set\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy metrics\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Print accuracy metrics\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"Mean Absolute Error:\", mae)\n",
    "print(\"R^2 Score:\", r2)\n",
    "\n",
    "\n",
    "# Create a DataFrame to store predicted marks along with ID, manual marks, and difference\n",
    "predictions_df = pd.DataFrame({'ID': X_test.index, 'Manual_Marks': y_test, 'Predicted_Marks': y_pred})\n",
    "predictions_df['Difference'] = predictions_df['Manual_Marks'] - predictions_df['Predicted_Marks']\n",
    "\n",
    "# Print the DataFrame\n",
    "print(predictions_df[['ID', 'Manual_Marks', 'Predicted_Marks', 'Difference']])\n",
    "print(len(predictions_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34e389b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"G:\\\\GitHub Classroom Assistant\\\\Topic Modelling Dataset\\\\TMC\\\\Formanualmarkass-1.csv\"\n",
    "mdata = pd.read_csv(file_path)\n",
    "\n",
    "# Define features and target variable\n",
    "X = mdata[['CommentType3', 'Comment_Category', 'Total_Answer_keywords_per']]\n",
    "y = mdata['Manual_Mark']\n",
    "\n",
    "# Convert categorical features to numerical using one-hot encoding\n",
    "X_encoded = pd.get_dummies(X)\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    \"Random Forest\": RandomForestRegressor(),\n",
    "    \"Gradient Boosting\": GradientBoostingRegressor(),\n",
    "    \"Support Vector Machines\": SVR(),\n",
    "    \"Linear Regression\": LinearRegression()\n",
    "}\n",
    "\n",
    "# Define scoring metric (mean squared error)\n",
    "scorer = make_scorer(mean_squared_error)\n",
    "\n",
    "# Evaluate models using cross-validation\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    cv_scores = cross_val_score(model, X_encoded, y, cv=5, scoring=scorer)\n",
    "    results[name] = cv_scores.mean()\n",
    "\n",
    "# Print results\n",
    "print(\"Algorithm\\tMean Squared Error\")\n",
    "print(\"---------------------------------\")\n",
    "for name, score in results.items():\n",
    "    print(f\"{name}\\t{score}\")\n",
    "\n",
    "# Select the best algorithm\n",
    "best_algorithm = min(results, key=results.get)\n",
    "print(f\"\\nBest Algorithm: {best_algorithm}\")\n",
    "\n",
    "# Train the best algorithm on the full dataset and make predictions\n",
    "best_model = models[best_algorithm]\n",
    "best_model.fit(X_encoded, y)\n",
    "predicted_marks = best_model.predict(X_encoded)\n",
    "\n",
    "# Round the predicted marks to the nearest integer (floor value)\n",
    "predicted_marks_rounded = np.floor(predicted_marks)\n",
    "\n",
    "# Print predicted marks with ID and manual marks\n",
    "print(\"\\nPredicted Marks (Rounded) with ID and Manual Marks:\")\n",
    "predicted_df = pd.DataFrame({\n",
    "    'ID': mdata['ID'],\n",
    "    'Manual_Mark': mdata['Manual_Mark'],\n",
    "    'Predicted_Mark': predicted_marks_rounded\n",
    "})\n",
    "print(predicted_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ed72e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, cohen_kappa_score\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"G:\\\\GitHub Classroom Assistant\\\\Topic Modelling Dataset\\\\TMC\\\\Formanualmarkass-1.csv\"\n",
    "mdata = pd.read_csv(file_path)\n",
    "\n",
    "# Select features and target variable\n",
    "X = mdata[['CommentType3', 'Comment_Category', 'Total_Answer_keywords_per']]\n",
    "y = mdata['Manual_Mark']\n",
    "\n",
    "# Convert categorical features to numerical using one-hot encoding\n",
    "X_encoded = pd.get_dummies(X)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(),\n",
    "    \"Support Vector Machine\": SVC(),\n",
    "    \"Logistic Regression\": LogisticRegression()\n",
    "}\n",
    "\n",
    "# Train and evaluate models\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    report = classification_report(y_test, y_pred, output_dict=True)\n",
    "    kappa = cohen_kappa_score(y_test, y_pred)\n",
    "    results[name] = {\"report\": report, \"kappa\": kappa}\n",
    "\n",
    "# Print results\n",
    "print(\"Algorithm\\tAccuracy\\tF1-Score\\tPrecision\\tRecall\\tCohen's Kappa\")\n",
    "print(\"------------------------------------------------------------------\")\n",
    "for name, result in results.items():\n",
    "    report = result['report']\n",
    "    accuracy = report['accuracy']\n",
    "    f1_score = report['weighted avg']['f1-score']\n",
    "    precision = report['weighted avg']['precision']\n",
    "    recall = report['weighted avg']['recall']\n",
    "    kappa = result['kappa']\n",
    "    print(f\"{name}\\t{accuracy:.4f}\\t\\t{f1_score:.4f}\\t\\t{precision:.4f}\\t\\t{recall:.4f}\\t\\t{kappa:.4f}\")\n",
    "\n",
    "# Select the best model based on Cohen's Kappa\n",
    "best_model_name = max(results, key=lambda x: results[x]['kappa'])\n",
    "best_model = models[best_model_name]\n",
    "\n",
    "# Predict marks with the best model\n",
    "best_model.fit(X_train, y_train)\n",
    "y_pred_best = best_model.predict(X_test)\n",
    "\n",
    "# Print ID, predicted marks, and manual marks with the best model\n",
    "print(\"\\nPredicted Marks with the Best Model:\")\n",
    "print(\"ID\\tPredicted_Marks\\tManual_Marks\")\n",
    "print(\"------------------------------------\")\n",
    "for idx, (pred_mark, manual_mark) in enumerate(zip(y_pred_best, y_test)):\n",
    "    print(f\"{idx}\\t{pred_mark}\\t\\t{manual_mark}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e923d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "# from sklearn.svm import SVR\n",
    "# from sklearn.linear_model import LinearRegression\n",
    "# from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# # Load the dataset\n",
    "# file_path = \"G:\\\\GitHub Classroom Assistant\\\\Topic Modelling Dataset\\\\TMC\\\\Formanualmarkass-1.csv\"\n",
    "# mdata = pd.read_csv(file_path)\n",
    "\n",
    "# # Select features and target variable\n",
    "# X = mdata[['CommentType3', 'Comment_Category', 'Total_Answer_keywords_per']]\n",
    "# y = mdata['Manual_Mark']\n",
    "\n",
    "# # Convert categorical features to numerical using one-hot encoding\n",
    "# X_encoded = pd.get_dummies(X)\n",
    "\n",
    "# # Split the dataset into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Initialize models\n",
    "# models = {\n",
    "#     'Random Forest': RandomForestRegressor(),\n",
    "#     'Gradient Boosting': GradientBoostingRegressor(),\n",
    "#     'Support Vector Machine': SVR(),\n",
    "#     'Linear Regression': LinearRegression()\n",
    "# }\n",
    "\n",
    "# # Train each model and calculate MSE\n",
    "# mse_results = {}\n",
    "# for name, model in models.items():\n",
    "#     model.fit(X_train, y_train)\n",
    "#     y_pred = model.predict(X_test)\n",
    "#     mse = mean_squared_error(y_test, y_pred)\n",
    "#     mse_results[name] = mse\n",
    "\n",
    "# # Print MSE values in tabular form\n",
    "# print(\"Algorithm\\t\\tMean Squared Error\")\n",
    "# print(\"--------------------------------------\")\n",
    "# for name, mse in mse_results.items():\n",
    "#     print(f\"{name.ljust(20)}\\t{mse:.4f}\")\n",
    "\n",
    "# # Find the best algorithm based on MSE\n",
    "# best_algorithm = min(mse_results, key=mse_results.get)\n",
    "# print(f\"\\nBest Algorithm: {best_algorithm}\")\n",
    "\n",
    "# # Print ID, manual mark, and predicted mark of the best algorithm\n",
    "# best_model = models[best_algorithm]\n",
    "# best_model.fit(X, y)  # Train on the full dataset\n",
    "# y_pred_best = best_model.predict(X)  # Predict on the full dataset\n",
    "# results = pd.DataFrame({'ID': data['ID'], 'Manual_Mark': data['Manual_Mark'], 'Predicted_Mark': y_pred_best})\n",
    "# print(\"\\nID\\tManual Mark\\tPredicted Mark\")\n",
    "# print(results)\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"G:\\\\GitHub Classroom Assistant\\\\Topic Modelling Dataset\\\\TMC\\\\Formanualmarkass-1.csv\"\n",
    "mdata = pd.read_csv(file_path)\n",
    "\n",
    "# Encode categorical variables\n",
    "encoder = LabelEncoder()\n",
    "mdata['CommentType3'] = encoder.fit_transform(mdata['CommentType3'])\n",
    "mdata['Comment_Category'] = encoder.fit_transform(mdata['Comment_Category'])\n",
    "\n",
    "# Select features and target variable\n",
    "X = mdata[['CommentType3', 'Comment_Category', 'Total_Answer_keywords_per']]\n",
    "y = mdata['Manual_Mark']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    \"Random Forest\": RandomForestRegressor(),\n",
    "    \"Gradient Boosting\": GradientBoostingRegressor(),\n",
    "    \"Support Vector Machine\": SVR(),\n",
    "    \"Linear Regression\": LinearRegression()\n",
    "}\n",
    "\n",
    "# Train models and calculate MSE\n",
    "mse_results = {}\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mse_results[name] = mse\n",
    "\n",
    "# Print MSE for each model\n",
    "print(\"Algorithm\\tMean Squared Error\")\n",
    "print(\"--------------------------------------\")\n",
    "for name, mse in mse_results.items():\n",
    "    print(f\"{name}\\t\\t{mse}\")\n",
    "\n",
    "# Find best model\n",
    "best_algorithm = min(mse_results, key=mse_results.get)\n",
    "print(f\"\\nBest Algorithm: {best_algorithm}\")\n",
    "\n",
    "# Train the best model on the full dataset\n",
    "best_model = models[best_algorithm]\n",
    "best_model.fit(X, y)\n",
    "\n",
    "# Predict marks for the full dataset\n",
    "y_pred_full = best_model.predict(X)\n",
    "\n",
    "# Print ID, manual mark, and predicted mark of the best algorithm\n",
    "results = pd.DataFrame({'ID': mdata['ID'], 'Manual_Mark': mdata['Manual_Mark'], 'Predicted_Mark': y_pred_full})\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38922bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, classification_report, cohen_kappa_score\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "\n",
    "# Load the dataset\n",
    "# Load the dataset\n",
    "file_path = \"G:\\\\GitHub Classroom Assistant\\\\Topic Modelling Dataset\\\\TMC\\\\Formanualmarkass-1.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Encode categorical variables\n",
    "encoder = LabelEncoder()\n",
    "data['CommentType3'] = encoder.fit_transform(data['CommentType3'])\n",
    "data['Comment_Category'] = encoder.fit_transform(data['Comment_Category'])\n",
    "\n",
    "# Select features and target variable\n",
    "X = data[['CommentType3', 'Comment_Category', 'Total_Answer_keywords_per']]\n",
    "y = data['Manual_Mark']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    \"Random Forest\": RandomForestRegressor(),\n",
    "    \"Gradient Boosting\": GradientBoostingRegressor(),\n",
    "    \"Support Vector Machine\": SVR(),\n",
    "    \"Linear Regression\": LinearRegression()\n",
    "}\n",
    "\n",
    "# Train models and calculate MSE\n",
    "mse_results = {}\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mse_results[name] = mse\n",
    "\n",
    "# # Evaluate models based on classification metrics\n",
    "# report_results = {}\n",
    "# for name, model in models.items():\n",
    "#     y_pred = model.predict(X_test)\n",
    "#     y_pred_floor = [int(pred) for pred in y_pred]\n",
    "#     report = classification_report(y_test, y_pred_floor, output_dict=True)\n",
    "#     kappa = cohen_kappa_score(y_test, y_pred_floor)\n",
    "#     report_results[name] = {'MSE': mse_results[name], 'Kappa': kappa, 'Report': report}\n",
    "\n",
    "# Print metrics for each model\n",
    "print(\"Algorithm\\tMSE\\tKappa\\tAccuracy\\tPrecision\\tRecall\\tF1-Score\")\n",
    "print(\"---------------------------------------------------------------------\")\n",
    "for name, results in report_results.items():\n",
    "    mse = results['MSE']\n",
    "    kappa = results['Kappa']\n",
    "    accuracy = results['Report']['accuracy']\n",
    "    precision = results['Report']['weighted avg']['precision']\n",
    "    recall = results['Report']['weighted avg']['recall']\n",
    "    f1_score = results['Report']['weighted avg']['f1-score']\n",
    "    print(f\"{name}\\t{mse:.4f}\\t{kappa:.4f}\\t{accuracy:.4f}\\t{precision:.4f}\\t{recall:.4f}\\t{f1_score:.4f}\")\n",
    "\n",
    "# Find best model based on kappa score\n",
    "best_algorithm = max(report_results, key=lambda x: report_results[x]['Kappa'])\n",
    "print(f\"\\nBest Algorithm based on Kappa: {best_algorithm}\")\n",
    "\n",
    "# Train the best model on the full dataset\n",
    "best_model = models[best_algorithm]\n",
    "best_model.fit(X, y)\n",
    "\n",
    "# Predict marks for the full dataset\n",
    "y_pred_full = best_model.predict(X)\n",
    "\n",
    "# Print ID, manual mark, and predicted mark of the best algorithm\n",
    "results = pd.DataFrame({'ID': data['ID'], 'Manual_Mark': data['Manual_Mark'], 'Predicted_Mark': y_pred_full})\n",
    "results['Predicted_Mark'] = results['Predicted_Mark'].apply(lambda x: int(x))  # Round to nearest integer\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b4f1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"G:\\\\GitHub Classroom Assistant\\\\Topic Modelling Dataset\\\\TMC\\\\Formanualmarkass-1.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# One-hot encode categorical variables\n",
    "X_encoded = pd.get_dummies(X)\n",
    "\n",
    "# Split the encoded dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize models\n",
    "models = {\n",
    "    \"Random Forest\": RandomForestRegressor(),\n",
    "    \"Gradient Boosting\": GradientBoostingRegressor(),\n",
    "    \"Support Vector Machine\": SVR(),\n",
    "    \"Linear Regression\": LinearRegression()\n",
    "}\n",
    "\n",
    "# # Calculate MSE for each model\n",
    "# mse_results = {}\n",
    "# for name, model in models.items():\n",
    "#     model.fit(X_train, y_train)\n",
    "#     y_pred = model.predict(X_test)\n",
    "#     mse = mean_squared_error(y_test, y_pred)\n",
    "#     mse_results[name] = mse\n",
    "\n",
    "# # Print MSE for each model\n",
    "# print(\"Algorithm\\t\\tMean Squared Error\")\n",
    "# print(\"------------------------------------------\")\n",
    "# for name, mse in mse_results.items():\n",
    "#     print(f\"{name}\\t\\t{mse:.4f}\")\n",
    "\n",
    "# # Find the best algorithm (minimum MSE)\n",
    "# best_algo = min(mse_results, key=mse_results.get)\n",
    "# print(\"\\nBest Algorithm:\", best_algo)\n",
    "\n",
    "# # Calculate R-squared\n",
    "# r_squared = r2_score(y_test, y_pred_best)\n",
    "\n",
    "# # Calculate Mean Absolute Error (MAE)\n",
    "# mae = mean_absolute_error(y_test, y_pred_best)\n",
    "\n",
    "# print(\"R-squared:\", r_squared)\n",
    "# print(\"Mean Absolute Error (MAE):\", mae)\n",
    "\n",
    "# Print ID, manual marks, and predicted marks for the best algorithm\n",
    "# best_model = models[best_algo]\n",
    "# best_model.fit(X_train, y_train)\n",
    "# y_pred_best = best_model.predict(X_test)\n",
    "# rounded_pred_marks = np.floor(y_pred_best)  # Round down to the nearest integer\n",
    "# results = pd.DataFrame({'ID': data['ID'], 'Manual_Mark': data['Manual_Mark'], 'Predicted_Mark': rounded_pred_marks})\n",
    "# print(\"\\nID\\tManual Mark\\tPredicted Mark\")\n",
    "# print(results)\n",
    "# Print ID, manual marks, and predicted marks for the best algorithm\n",
    "# best_model = models[best_algo]\n",
    "# best_model.fit(X_train, y_train)\n",
    "# y_pred_best = best_model.predict(X_test)\n",
    "# rounded_pred_marks = np.floor(y_pred_best) # Round down to the nearest integer\n",
    "# results = pd.DataFrame({'ID': X_test.index, 'Manual_Mark': y_test, 'Predicted_Mark': rounded_pred_marks})\n",
    "# print(\"\\nID\\tManual Mark\\tPredicted Mark\")\n",
    "# print(results)\n",
    "\n",
    "\n",
    "\n",
    "# Initialize dictionaries to store R-squared and MAE for each model\n",
    "r_squared_results = {}\n",
    "mae_results = {}\n",
    "\n",
    "# Calculate R-squared and MAE for each model\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r_squared = r2_score(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mse_results[name] = mse\n",
    "    r_squared_results[name] = r_squared\n",
    "    mae_results[name] = mae\n",
    "\n",
    "# Print MSE, R-squared, and MAE for each model\n",
    "print(\"Algorithm\\t\\tMean Squared Error\\tR-squared\\t\\tMean Absolute Error\")\n",
    "print(\"---------------------------------------------------------------------------\")\n",
    "for name, mse in mse_results.items():\n",
    "    print(f\"{name}\\t\\t{mse:.4f}\\t\\t{r_squared_results[name]:.4f}\\t\\t{mae_results[name]:.4f}\")\n",
    "\n",
    "# Find the best algorithm based on MSE, R-squared, and MAE\n",
    "best_algo_mse = min(mse_results, key=mse_results.get)\n",
    "best_algo_r_squared = max(r_squared_results, key=r_squared_results.get)\n",
    "best_algo_mae = min(mae_results, key=mae_results.get)\n",
    "\n",
    "print(\"\\nBest Algorithm based on MSE:\", best_algo_mse)\n",
    "print(\"Best Algorithm based on R-squared:\", best_algo_r_squared)\n",
    "print(\"Best Algorithm based on MAE:\", best_algo_mae)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00125be6",
   "metadata": {},
   "source": [
    "### As we find linear regression model is best. now we write code to train this model and save output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0608c6",
   "metadata": {},
   "source": [
    "### <font color=\"Blue\"> TRAIN MODEL & SAVE (LINEAR REGRESSION) </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b886507",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import joblib\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"G:\\\\GitHub Classroom Assistant\\\\Topic Modelling Dataset\\\\TMC\\\\Formanualmarkass-1.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Extract features and target variable\n",
    "X = data[['CommentType3', 'Comment_Category', 'Total_Answer_keywords_per']]\n",
    "y = data['Manual_Mark']\n",
    "\n",
    "# One-hot encode categorical variables\n",
    "X_encoded = pd.get_dummies(X)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the Linear Regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Save the trained model to a file\n",
    "model_file_path = \"G:\\\\GitHub Classroom Assistant\\\\Topic Modelling Dataset\\\\TMC\\\\linear_regression_model.joblib\"\n",
    "joblib.dump(model, model_file_path)\n",
    "\n",
    "print(\"Linear Regression model saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2226991",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import joblib\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"G:\\\\GitHub Classroom Assistant\\\\Topic Modelling Dataset\\\\TMC\\\\Formanualmarkass-1.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Extract features and target variable\n",
    "X = data[['CommentType3', 'Comment_Category', 'Total_Answer_keywords_per']]\n",
    "y = data['Manual_Mark']\n",
    "\n",
    "# One-hot encode categorical variables\n",
    "encoder = OneHotEncoder()\n",
    "X_encoded = encoder.fit_transform(X)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the Linear Regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Save the trained model and encoder to a file\n",
    "model_file_path = \"G:\\\\GitHub Classroom Assistant\\\\Topic Modelling Dataset\\\\TMC\\\\linear_regression_model1.joblib\"\n",
    "joblib.dump((model, encoder), model_file_path)\n",
    "\n",
    "print(\"Linear Regression model and encoder saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6e8c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Assuming your data is loaded into a pandas DataFrame called 'data' with columns:\n",
    "# 'comment_id', 'ML_mark', 'rule_based_mark' (or any column containing human-assigned marks)\n",
    "\n",
    "# Calculate absolute difference between marks\n",
    "data['mark_difference'] = abs(data['ML_Mark'] - data['Manual_Mark'])\n",
    "\n",
    "# Binarize the difference for accuracy calculation (correct if difference is 0)\n",
    "data['correct'] = np.where(data['mark_difference'] == 0, 1, 0)  # 1 for correct prediction (0 difference)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(data['correct'], np.ones(len(data)))  # Ones represent all expected correct predictions\n",
    "\n",
    "print(\"Accuracy between Manual Marks and Training Model Predictions:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0faa38b",
   "metadata": {},
   "source": [
    "### <FONT COLOR=\"BLUE\">MODEL EVALUATION USING MSE, MAE AND R2</FONT>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e9bfd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"G:\\\\GitHub Classroom Assistant\\\\Topic Modelling Dataset\\\\TMC\\\\Formanualmarkass-1.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Extract actual manual marks and ML predicted marks\n",
    "actual_marks = data['Manual_Mark']\n",
    "predicted_marks = data['ML_Mark']\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "mse = mean_squared_error(actual_marks, predicted_marks)\n",
    "r2 = r2_score(actual_marks, predicted_marks)\n",
    "mae = mean_absolute_error(actual_marks, predicted_marks)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Evaluation Metrics:\")\n",
    "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
    "print(f\"R-squared (R2): {r2:.2f}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c21aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "badef91e",
   "metadata": {},
   "source": [
    "### <font color=\"Blue\"> APPLY MODEL & SAVE (LINEAR REGRESSION) </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9c4a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import joblib\n",
    "\n",
    "# Load the comment dataset\n",
    "comment_file_path = \"G:\\\\GitHub Classroom Assistant\\\\Topic Modelling Dataset\\\\TMC\\\\Formanualmarkass-12.csv\"\n",
    "comment_data = pd.read_csv(comment_file_path)\n",
    "\n",
    "# Assuming the comment dataset has the same features as the training dataset\n",
    "# Extract features\n",
    "X_comments = comment_data[['CommentType3', 'Comment_Category', 'Total_Answer_keywords_per']]\n",
    "\n",
    "# Handle missing values\n",
    "X_comments['Comment_Category'].fillna('No Match', inplace=True)  # Fill missing values with 'Unknown'\n",
    "\n",
    "# Load the saved Linear Regression model and encoder\n",
    "model_file_path = \"G:\\\\GitHub Classroom Assistant\\\\Topic Modelling Dataset\\\\TMC\\\\linear_regression_model1.joblib\"\n",
    "model, encoder = joblib.load(model_file_path)\n",
    "\n",
    "# One-hot encode categorical variables using the loaded encoder\n",
    "X_comments_encoded = encoder.transform(X_comments)\n",
    "\n",
    "# Predict marks for the comment dataset\n",
    "predicted_marks = model.predict(X_comments_encoded)\n",
    "\n",
    "# Store predicted marks in a new column \"ML_Mark\"\n",
    "comment_data['ML_Mark'] = np.ceil(predicted_marks)\n",
    "\n",
    "# Print student ID, name, comment, and predicted marks\n",
    "print(\"Student ID\\tName\\tComment\\tPredicted Marks\")\n",
    "print(\"----------------------------------------------\")\n",
    "for index, row in comment_data.iterrows():\n",
    "    print(f\"{row['ID']}\\t{row['Name']}\\t{row['ML_Mark']}\")\n",
    "\n",
    "# Rewrite the CSV file with the \"ML_Mark\" column\n",
    "comment_data.to_csv(comment_file_path, index=False)\n",
    "\n",
    "print(\"CSV file rewritten with ML_Mark column.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e435142b",
   "metadata": {},
   "source": [
    "### <FONT COLOR='BLUE'>RULE BASED METHOD TO ASSIGN MARK (NEW FORMULA)</FONT>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3ac2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define teacher keywords (modify as needed)\n",
    "teacher_keywords = ['variable', 'declaration', 'integer', 'undeclare', 'declare', 'datatype']\n",
    "\n",
    "# Define the rule-based mapping for marks based on comment details (modify as needed)\n",
    "def assign_marks(comment_type, comment_category, total_keywords, num_words):\n",
    "  \"\"\"\n",
    "  Assigns marks based on the provided rules and comment details.\n",
    "\n",
    "  Args:\n",
    "      comment_type: The type of comment (e.g., Detail, Basic).\n",
    "      comment_category: The category of the comment (e.g., Both Error & Solution).\n",
    "      total_keywords: The total number of teacher keywords matched in the comment.\n",
    "      num_words: The total number of words in the comment (optional for keyword percentage).\n",
    "\n",
    "  Returns:\n",
    "      An integer representing the assigned mark.\n",
    "  \"\"\"\n",
    "\n",
    "  if comment_type == \"Detail\":\n",
    "    if comment_category == \"Both Error and Solution\":\n",
    "      if total_keywords >= 70:  # Check keyword percentage\n",
    "        return 30\n",
    "      elif total_keywords >= 50 and total_keywords < 70:\n",
    "        return 25\n",
    "      elif total_keywords >= 30 and total_keywords <50:\n",
    "        return 20\n",
    "      elif total_keywords >= 10 and total_keywords > 30:\n",
    "        return 15\n",
    "      else:\n",
    "        return 0\n",
    "    elif comment_category in [\"Error Only\", \"Solution Only\"]:\n",
    "      if total_keywords >= 70:  # Check keyword percentage\n",
    "        return 22\n",
    "      elif total_keywords >= 50 and total_keywords < 70:\n",
    "        return 18\n",
    "      elif total_keywords >= 30 and total_keywords <50:\n",
    "        return 14\n",
    "      elif total_keywords >= 10 and total_keywords > 30:\n",
    "        return 10\n",
    "      else:\n",
    "        return 0\n",
    "    elif comment_category=='No Match':\n",
    "      \n",
    "      if total_keywords >= 70:  # Check keyword percentage\n",
    "        return 20\n",
    "      elif total_keywords >= 50 and total_keywords < 70:\n",
    "        return 16\n",
    "      elif total_keywords >= 30 and total_keywords <50:\n",
    "        return 12\n",
    "      elif total_keywords >= 10 and total_keywords > 30:\n",
    "        return 8\n",
    "      else:\n",
    "        return 0\n",
    "    \n",
    "  elif comment_type == \"Basic\":\n",
    "    if comment_category == \"Both Error and Solution\":\n",
    "      if total_keywords >= 70:  # Check keyword percentage\n",
    "        return 22\n",
    "      elif total_keywords >= 50 and total_keywords < 70:\n",
    "        return 18\n",
    "      elif total_keywords >= 30 and total_keywords <50:\n",
    "        return 14\n",
    "      elif total_keywords >= 10 and total_keywords > 30:\n",
    "        return 10\n",
    "      else:\n",
    "        return 0\n",
    "    elif comment_category in [\"Error Only\", \"Solution Only\"]:\n",
    "      if total_keywords >= 70:  # Check keyword percentage\n",
    "        return 18\n",
    "      elif total_keywords >= 50 and total_keywords < 70:\n",
    "        return 14\n",
    "      elif total_keywords >= 30 and total_keywords <50:\n",
    "        return 10\n",
    "      elif total_keywords >= 10 and total_keywords > 30:\n",
    "        return 6\n",
    "      else:\n",
    "        return 0\n",
    "    elif comment_category=='No Match':\n",
    "      \n",
    "      if total_keywords >= 70:  # Check keyword percentage\n",
    "        return 15\n",
    "      elif total_keywords >= 50 and total_keywords < 70:\n",
    "        return 12\n",
    "      elif total_keywords >= 30 and total_keywords <50:\n",
    "        return 8\n",
    "      elif total_keywords >= 10 and total_keywords >30:\n",
    "        return 3\n",
    "      else:\n",
    "        return 0\n",
    "    \n",
    "# Load the comment dataset\n",
    "comment_file_path = \"G:\\\\GitHub Classroom Assistant\\\\Topic Modelling Dataset\\\\TMC\\\\Formanualmarkass-12.csv\"\n",
    "comment_data = pd.read_csv(comment_file_path)\n",
    "\n",
    "# Apply the rule-based assignment to the DataFrame\n",
    "comment_data[\"Rule_Based_Mark\"] = comment_data.apply(lambda row: assign_marks(row[\"CommentType3\"], row[\"Comment_Category\"], row['Total_Answer_keywords_per'], len(row[\"CorrectComment3_Spell\"].split())), axis=1)\n",
    "comment_data[\"Mark_Difference\"] = comment_data.apply(lambda row: row[\"ML_Mark\"]-row[\"Rule_Based_Mark\"], axis=1)\n",
    "\n",
    "print(comment_data[\"Rule_Based_Mark\"])\n",
    "\n",
    "comment_data.to_csv(comment_file_path, index=False)\n",
    "\n",
    "print(\"CSV file rewritten with ML_Mark column.\")\n",
    "\n",
    "# Print a sample of the marked DataFrame (optional)\n",
    "print(comment_data[[\"ID\",\"Comment_Category\",\"CorrectComment3_Spell\", \"Rule_Based_Mark\",\"ML_Mark\"]].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f56ac77",
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_data [[\"ID\",\"Name\",\"ML_Mark\",\"Rule_Based_Mark\",\"Mark_Difference\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a062a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_data[\"Mark_Difference\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775680f6",
   "metadata": {},
   "source": [
    "### <FONT COLOR=\"BLUE\">VISUALIZATION : HISTOGRAM FOR MARKS DIFFERENCE </FONT>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68d8505",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "plt.hist(comment_data['Mark_Difference'], bins=10, edgecolor='black')\n",
    "plt.xlabel('Difference between ML Mark and Rule-Based Mark')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Mark Difference')\n",
    "plt.grid(axis='y')\n",
    "plt.show()\n",
    "\n",
    "# Create boxplot\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.boxplot(comment_data['Mark_Difference'])\n",
    "plt.xlabel('Mark Difference')\n",
    "plt.ylabel('Mark Difference')\n",
    "plt.title('Boxplot of Mark Difference')\n",
    "plt.grid(axis='y')\n",
    "plt.show()\n",
    "\n",
    "print(\"Summary Statistics of Mark Difference:\")\n",
    "print(comment_data['Mark_Difference'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df240bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab2fb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_data.Mark_Difference.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abcc73da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate percentage difference for each observation\n",
    "comment_data['Percentage_Difference'] = ((comment_data['ML_Mark'] - comment_data['Rule_Based_Mark']) / comment_data['Rule_Based_Mark']) * 100\n",
    "\n",
    "# Compute the average percentage difference\n",
    "average_percentage_difference = comment_data['Percentage_Difference'].mean()\n",
    "\n",
    "print(\"Average Percentage Difference:\", average_percentage_difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b511543",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e758571c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "# Define the data path\n",
    "data_path = \"G:\\\\GitHub Classroom Assistant\\\\Topic Modelling Dataset\\\\TMC\\\\Formanualmarkass-12.csv\"\n",
    "\n",
    "# Read the data from the CSV file\n",
    "comment_data = pd.read_csv(data_path)\n",
    "\n",
    "# Option 1: Select only numerical features\n",
    "numerical_features = ['Total_Answer_keywords_per', \"ML_Mark\", \"Rule_Based_Mark\"]\n",
    "corr_matrix = comment_data[numerical_features].corr()\n",
    "\n",
    "# Option 2: Convert appropriate features to numerical (if applicable)\n",
    "# Assuming 'CommentType3' and 'Comment_Category' have categories that can be mapped to numbers\n",
    "category_mapping = {'Basic': 1, 'Detail': 2, 'Excellent': 3}  # Define mapping for your categories\n",
    "comment_data['CommentType3_Num'] = comment_data['CommentType3'].replace(category_mapping)\n",
    "comment_data['Comment_Category_Num'] = comment_data['Comment_Category'].replace(category_mapping)\n",
    "\n",
    "features = ['CommentType3_Num', 'Comment_Category_Num', 'Total_Answer_keywords_per', \"ML_Mark\", \"Rule_Based_Mark\"]\n",
    "corr_matrix = comment_data[features].corr()\n",
    "\n",
    "# Generate the heatmap\n",
    "sns.heatmap(corr_matrix, annot=True)  # Set annot=True to display correlation values on the heatmap\n",
    "plt.title(\"Correlation between Features\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a36f95",
   "metadata": {},
   "source": [
    "### <font color=\"Blue\"> ADJUST MARK BY APPLYING WEIGHT </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacda72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = 0.5  # Adjust this value as needed\n",
    "\n",
    "# Apply weighted adjustment to predicted marks\n",
    "comment_data['Adjusted_Marks'] = comment_data.apply(lambda row: weighted_adjustment(row['ML_Mark'], row['Rule_Based_Mark'], weight), axis=1)\n",
    "\n",
    "# Calculate average marks\n",
    "comment_data['Avg_Mark'] = np.ceil((comment_data['ML_Mark'] + comment_data['Rule_Based_Mark']) / 2)\n",
    "\n",
    "# Print the first few rows of the dataframe with the new columns\n",
    "print(comment_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fde8c99",
   "metadata": {},
   "source": [
    "### <font color=\"blue\"> VISUALIZATION AFTER ADJUSTMENT </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ddfdd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "plt.hist(comment_data['Avg_Mark'], bins=10, edgecolor='black')\n",
    "plt.xlabel('Difference between ML Mark and Rule-Based Mark')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Mark Difference')\n",
    "plt.grid(axis='y')\n",
    "plt.show()\n",
    "\n",
    "# Create boxplot\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.boxplot(comment_data['Avg_Mark'])\n",
    "plt.xlabel('Mark Difference')\n",
    "plt.ylabel('Mark Difference')\n",
    "plt.title('Boxplot of Mark Difference')\n",
    "plt.grid(axis='y')\n",
    "plt.show()\n",
    "\n",
    "print(\"Summary Statistics of Mark Difference:\")\n",
    "print(comment_data['Avg_Mark'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1260609",
   "metadata": {},
   "source": [
    "### <font color=\"Blue\"> FIND ACCURACY </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f07be2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Calculate metrics for ML method\n",
    "ml_mse = mean_squared_error(comment_data['Actual_Manual_Marks'], comment_data['ML_Mark'])\n",
    "ml_mae = mean_absolute_error(comment_data['Actual_Manual_Marks'], comment_data['ML_Mark'])\n",
    "ml_r2 = r2_score(comment_data['Actual_Manual_Marks'], comment_data['ML_Mark'])\n",
    "\n",
    "# Calculate metrics for rule-based method\n",
    "rule_based_mse = mean_squared_error(comment_data['Actual_Manual_Marks'], comment_data['Rule_Based_Mark'])\n",
    "rule_based_mae = mean_absolute_error(comment_data['Actual_Manual_Marks'], comment_data['Rule_Based_Mark'])\n",
    "rule_based_r2 = r2_score(comment_data['Actual_Manual_Marks'], comment_data['Rule_Based_Mark'])\n",
    "\n",
    "# Print metrics\n",
    "print(\"Metrics for ML Method:\")\n",
    "print(\"MSE:\", ml_mse)\n",
    "print(\"MAE:\", ml_mae)\n",
    "print(\"R-squared:\", ml_r2)\n",
    "print()\n",
    "\n",
    "print(\"Metrics for Rule-Based Method:\")\n",
    "print(\"MSE:\", rule_based_mse)\n",
    "print(\"MAE:\", rule_based_mae)\n",
    "print(\"R-squared:\", rule_based_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbfafec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Define the data path\n",
    "data_path = \"G:\\\\GitHub Classroom Assistant\\\\Topic Modelling Dataset\\\\TMC\\\\Formanualmarkass-1.csv\"\n",
    "\n",
    "# Read the data from the CSV file\n",
    "comment_data = pd.read_csv(data_path)\n",
    "\n",
    "# Assuming the marks are in columns named 'ML_Mark', 'Rule_Based_Mark', and 'Actual_Mark'\n",
    "# (modify these column names if they differ in your file)\n",
    "\n",
    "# Calculate absolute difference for both methods\n",
    "comment_data['ML_Diff'] = abs(comment_data['ML_Mark'] - comment_data['Manual_Mark'])\n",
    "comment_data['Rule_Diff'] = abs(comment_data['Rule_Based_Mark'] - comment_data['Manual_Mark'])\n",
    "\n",
    "# Accuracy for ML Model (within +/- 2 marks)\n",
    "ml_accuracy = (comment_data['ML_Diff'] <= 2).sum() / len(comment_data)\n",
    "\n",
    "# Accuracy for Rule-Based Method (within +/- 2 marks)\n",
    "rule_accuracy = (comment_data['Rule_Diff'] <= 2).sum() / len(comment_data)\n",
    "\n",
    "# Calculate agreement metrics\n",
    "mae_ml = mean_absolute_error(comment_data['Manual_Mark'], comment_data['ML_Mark'])\n",
    "mae_rule = mean_absolute_error(comment_data['Manual_Mark'], comment_data['Rule_Based_Mark'])\n",
    "\n",
    "mse_ml = mean_squared_error(comment_data['Manual_Mark'], comment_data['ML_Mark'])\n",
    "mse_rule = mean_squared_error(comment_data['Manual_Mark'], comment_data['Rule_Based_Mark'])\n",
    "\n",
    "corr_ml, _ = pearsonr(comment_data['Manual_Mark'], comment_data['ML_Mark'])\n",
    "corr_rule, _ = pearsonr(comment_data['Manual_Mark'], comment_data['Rule_Based_Mark'])\n",
    "\n",
    "# Print the results\n",
    "print(\"Accuracy:\")\n",
    "print(f\"ML Model Accuracy: {ml_accuracy:.2f}\")\n",
    "print(f\"Rule-Based Accuracy: {rule_accuracy:.2f}\")\n",
    "\n",
    "print(\"\\nAgreement Metrics:\")\n",
    "print(f\"MAE (ML): {mae_ml:.2f}\")\n",
    "print(f\"MAE (Rule-Based): {mae_rule:.2f}\")\n",
    "print(f\"MSE (ML): {mse_ml:.2f}\")\n",
    "print(f\"MSE (Rule-Based): {mse_ml:.2f}\")\n",
    "print(f\"Correlation (ML): {corr_ml:.2f}\")\n",
    "print(f\"Correlation (Rule-Based): {corr_rule:.2f}\")\n",
    "\n",
    "# Compare metrics to determine better performing method\n",
    "\n",
    "if ml_accuracy > rule_accuracy and (mae_ml < mae_rule or corr_ml > corr_rule):\n",
    "  print(\"\\nML Model performs better based on accuracy and agreement metrics.\")\n",
    "elif rule_accuracy > ml_accuracy and (mae_rule < mae_ml or corr_rule > corr_ml):\n",
    "  print(\"\\nRule-Based Method performs better based on accuracy and agreement metrics.\")\n",
    "else:\n",
    "  print(\"\\nThe performance is comparable. Consider further analysis or domain knowledge to decide.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a1a296",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the data path\n",
    "data_path = \"G:\\\\GitHub Classroom Assistant\\\\Topic Modelling Dataset\\\\TMC\\\\Formanualmarkass-12.csv\"\n",
    "\n",
    "# Read the data from the CSV file\n",
    "comment_data = pd.read_csv(data_path)\n",
    "\n",
    "# Assuming the marks are in columns named 'ML_Mark' and 'Rule_Based_Mark'\n",
    "# (modify these column names if they differ in your file)\n",
    "\n",
    "# Calculate the median for each method's predictions\n",
    "ml_median = comment_data['ML_Mark'].median()\n",
    "rule_based_median = comment_data['Rule_Based_Mark'].median()\n",
    "\n",
    "# Calculate the absolute deviations from the median for each method\n",
    "comment_data['ML_Abs_Dev'] = abs(comment_data['ML_Mark'] - ml_median)\n",
    "comment_data['Rule_Based_Abs_Dev'] = abs(comment_data['Rule_Based_Mark'] - rule_based_median)\n",
    "\n",
    "# Calculate the median absolute deviation (MAD) for each method\n",
    "ml_mad = comment_data['ML_Abs_Dev'].median()\n",
    "rule_based_mad = comment_data['Rule_Based_Abs_Dev'].median()\n",
    "\n",
    "# Determine the method with lower MAD\n",
    "if ml_mad < rule_based_mad:\n",
    "    final_marks = comment_data['ML_Mark']\n",
    "    print(\"ML Model predictions used for final marks due to lower MAD.\")\n",
    "else:\n",
    "    final_marks = comment_data['Rule_Based_Mark']\n",
    "    print(\"Rule-Based System predictions used for final marks due to lower MAD.\")\n",
    "\n",
    "# Print the final marks (assuming you want to see them)\n",
    "print(\"\\nFinal Marks:\")\n",
    "print(final_marks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5a3a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Define the data path\n",
    "data_path = \"G:\\\\GitHub Classroom Assistant\\\\Topic Modelling Dataset\\\\TMC\\\\Formanualmarkass-1.csv\"\n",
    "\n",
    "# Read the data from the CSV file\n",
    "comment_data = pd.read_csv(data_path)\n",
    "\n",
    "# Assuming the marks are in columns named 'ML_Mark', 'Rule_Based_Mark', and 'Actual_Mark'\n",
    "# (modify these column names if they differ in your file)\n",
    "\n",
    "# Calculate absolute differences for both methods\n",
    "comment_data['ML_Diff'] = abs(comment_data['ML_Mark'] - comment_data['Manual_Mark'])\n",
    "comment_data['Rule_Diff'] = abs(comment_data['Rule_Based_Mark'] - comment_data['Manual_Mark'])\n",
    "\n",
    "# Cost per unit of overprediction and underprediction (adjust these values based on your context)\n",
    "cost_overprediction_ml = 1  # Hypothetical cost for ML model overprediction\n",
    "cost_underprediction_ml = 2  # Hypothetical cost for ML model underprediction\n",
    "\n",
    "cost_overprediction_rule = 2  # Hypothetical cost for rule-based overprediction\n",
    "cost_underprediction_rule = 1  # Hypothetical cost for rule-based underprediction\n",
    "\n",
    "# Calculate cost of errors for both methods\n",
    "comment_data['ML_Cost'] = comment_data['ML_Diff'] * (\n",
    "    np.where(comment_data['ML_Diff'] > 0, cost_overprediction_ml, cost_underprediction_ml)\n",
    ")\n",
    "comment_data['Rule_Cost'] = comment_data['Rule_Diff'] * (\n",
    "    np.where(comment_data['Rule_Diff'] > 0, cost_overprediction_rule, cost_underprediction_rule)\n",
    ")\n",
    "\n",
    "# Calculate total cost for each method\n",
    "total_cost_ml = comment_data['ML_Cost'].sum()\n",
    "total_cost_rule = comment_data['Rule_Cost'].sum()\n",
    "\n",
    "# Determine the method with lower total cost\n",
    "if total_cost_ml < total_cost_rule:\n",
    "    final_marks = comment_data['ML_Mark']\n",
    "    print(\n",
    "        \"ML Model predictions used for final marks due to lower total cost of errors.\"\n",
    "    )\n",
    "else:\n",
    "    final_marks = comment_data['Rule_Based_Mark']\n",
    "    print(\n",
    "        \"Rule-Based System predictions used for final marks due to lower total cost of errors.\"\n",
    "    )\n",
    "\n",
    "# Print the final marks (assuming you want to see them)\n",
    "print(\"\\nFinal Marks:\")\n",
    "print(final_marks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9cd18c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# # Define the total marks\n",
    "# total_marks = 50\n",
    "\n",
    "# # Define the criteria and corresponding marks allocation\n",
    "# marks_criteria = {\n",
    "# #     'Cosine_Value_Keywords': {'High': 15, 'Low': 10},\n",
    "#     'CommentType3': {'Detail': 10, 'Basic': 5},\n",
    "#     'Comment_Category': {'Both Error and Solution': 20, 'Solution Only': 10, 'Error Only': 10, 'None': 2},\n",
    "#     'Total_Answer_Keyword2': {'>4': 20, '=4': 15, '<4': 10, '=0': 1}\n",
    "# }\n",
    "# teacher_keywords = ['variable', 'declaration', 'integer', 'int', 'declare','datatype',\"data\",\"type\"]\n",
    "# lenanskeyword= len(teacher_keywords)\n",
    "\n",
    "# # Function to assign marks based on the criteria\n",
    "# def assign_marks(row):\n",
    "#     marks = 0\n",
    "#     assignmarks=[]\n",
    "#     for criterion, allocation in marks_criteria.items():\n",
    "#         value = row[criterion]\n",
    "# #         print(f\"{value}-->{allocation}\")\n",
    "        \n",
    "#         if value in allocation:\n",
    "#             marks += allocation[value]\n",
    "#             assignmarks.append(allocation[value])\n",
    "#         elif value >= (lenanskeyword)*0.60:\n",
    "# #             print(f\"{value}-->{(lenanskeyword)*0.60}\")\n",
    "#             marks += 15\n",
    "#             assignmarks.append(value)\n",
    "#         elif value <= (lenanskeyword)*0.60:\n",
    "# #             print(f\"{value}-->{(lenanskeyword)*0.60}\")\n",
    "#             marks += 10\n",
    "#             assignmarks.append(value)\n",
    "#         elif value == 0:\n",
    "# #             print(f\"{value}-->{(lenanskeyword)*0.60}\")\n",
    "#             assignmarks.append(value)\n",
    "#             marks += 0\n",
    "#         else:\n",
    "#              marks += 0  # If criterion not found, assign 0 marks\n",
    "# #         print(assignmarks)\n",
    "#     return marks\n",
    "# # Apply the function to the DataFrame to calculate the marks for each comment\n",
    "# newdata['Mark_With_3Rule'] = newdata.apply(assign_marks, axis=1)\n",
    "\n",
    "# # Display the updated DataFrame with marks\n",
    "# print(newdata[[\"ID\",\"Name\",\"Mark_With_3Rule\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76715271",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming your data is loaded into a pandas DataFrame called 'data' with columns:\n",
    "# 'comment_id', 'ML_mark', 'rule_based_mark'\n",
    "\n",
    "# Calculate absolute difference in marks for highlighting discrepancies\n",
    "comment_data['mark_difference'] = abs(comment_data['ML_mark'] - comment_data['rule_based_mark'])\n",
    "\n",
    "# Define colors based on difference threshold (adjust threshold as needed)\n",
    "colors = ['blue' if diff < 2 else 'red' for diff in data['mark_difference']]\n",
    "\n",
    "# Create the scatter plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(comment_data['ID'], comment_data['ML_mark'], label='ML Mark', color=colors)\n",
    "plt.scatter(comment_data['ID'], comment_data['rule_based_mark'], label='Rule-Based Mark')\n",
    "\n",
    "# Add trendline for ML marks\n",
    "m, b = np.polyfit(comment_data['ID'], comment_data['ML_mark'], 1)\n",
    "plt.plot(comment_data['ID'], m * comment_data['ID'] + b, color='green', linestyle='--', label='ML Trendline')\n",
    "\n",
    "# Labels, title, and legend\n",
    "plt.xlabel('Comment ID')\n",
    "plt.ylabel('Mark')\n",
    "plt.title('Comparison of Assigned Marks')\n",
    "plt.legend()\n",
    "\n",
    "# Highlight discrepancies in the legend (optional)\n",
    "handles, labels = plt.gca().get_legend_handles_labels()\n",
    "by_label = dict(zip(labels, handles))\n",
    "plt.legend(by_label.values(), by_label.keys())\n",
    "\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7213c6fd",
   "metadata": {},
   "source": [
    "### <FONT COLOR=\"BLUE\"> ASSIGN MARKS USING COSINE SIMILARITY </FONT>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86771f01",
   "metadata": {},
   "source": [
    "#### <FONT COLOR=\"RED\"> USING WORD2VEC</FONT>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdcd0933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from gensim.models import Word2Vec  # Import Word2Vec if not already available\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# # Sample teacher's answer (assuming it's the same for all comments)\n",
    "# teacher_answer = \"Forget to declare datatype of variable during declaration. So it generate syntax error. We have to declare variable integer because in printf statement it use %d as format specifier.\"\n",
    "\n",
    "# # Load a pre-trained Word2Vec model (replace with the path to your model)\n",
    "# model = Word2Vec.load(\"path/to/your/word2vec.model\")\n",
    "\n",
    "# def calculate_cosine_similarity(comment, model, teacher_keywords):\n",
    "#   \"\"\"\n",
    "#   Calculates cosine similarity between a comment and teacher keywords.\n",
    "\n",
    "#   Args:\n",
    "#       comment: The student's comment (string).\n",
    "#       model: The loaded Word2Vec model.\n",
    "#       teacher_keywords: A list of teacher answer keywords (strings).\n",
    "\n",
    "#   Returns:\n",
    "#       The cosine similarity score between the comment and teacher keywords (float).\n",
    "#   \"\"\"\n",
    "#   # Pre-process comment (optional: lowercase, remove punctuation)\n",
    "#   comment = comment.lower()\n",
    "#   comment_embedding = sentence_embedding(comment, model)\n",
    "\n",
    "#   # Create a document vector representing teacher keywords (average of word vectors)\n",
    "#   teacher_embedding = np.mean([model.wv[word] for word in teacher_keywords if word in model.wv.vocab], axis=0)\n",
    "\n",
    "#   # Calculate cosine similarity\n",
    "#   return cosine_similarity(comment_embedding.reshape(1, -1), teacher_embedding.reshape(1, -1))[0][0]\n",
    "\n",
    "# # Function to generate sentence embedding (same as previous example)\n",
    "# def sentence_embedding(sentence, model):\n",
    "#   # ... (same implementation as before)\n",
    "\n",
    "# # Assuming your DataFrame is called 'data' with a column 'cleancomment3'\n",
    "# def assign_marks_with_cosine(data, teacher_answer, teacher_keywords):\n",
    "#   \"\"\"\n",
    "#   Assigns marks based on cosine similarity and predefined thresholds.\n",
    "\n",
    "#   Args:\n",
    "#       data: The pandas DataFrame containing comments.\n",
    "#       teacher_answer: The teacher's answer (string).\n",
    "#       teacher_keywords: A list of teacher answer keywords (strings).\n",
    "\n",
    "#   Returns:\n",
    "#       A modified DataFrame with a new column 'Marks_With_Cosine' containing marks.\n",
    "#   \"\"\"\n",
    "#   data['Marks_With_Cosine'] = data['cleancomment3'].apply(lambda comment: calculate_cosine_similarity(comment, model, teacher_keywords))\n",
    "#   data['Marks_With_Cosine'] = data['Marks_With_Cosine'].apply(lambda score: \n",
    "#                                                          50 if score > 0.7 else (\n",
    "#                                                              35 if score > 0.5 else (\n",
    "#                                                                  30 if score > 0.2 else 10\n",
    "#                                                              )\n",
    "#                                                          )\n",
    "#                                                        )\n",
    "#   return data\n",
    "\n",
    "# # Apply the function to your DataFrame\n",
    "# data = assign_marks_with_cosine(data.copy(), teacher_answer, teacher_keywords)\n",
    "\n",
    "# # Print the DataFrame with the new 'Marks_With_Cosine' column (optional)\n",
    "# print(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180937d8",
   "metadata": {},
   "source": [
    "#### <FONT COLOR=\"RED\">USING COUNTVECTORIZE</FONT>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4be942f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_marks_with_countvectorizer(data, threshold_high=0.7, threshold_medium=0.5, marks_high=50, marks_medium=35, marks_low=10):\n",
    "  \"\"\"\n",
    "  Assigns marks based on cosine similarity with CountVectorizer and predefined thresholds.\n",
    "\n",
    "  Args:\n",
    "      data: The pandas DataFrame containing comments and \"Cosine_Similarity_Keywords\" column.\n",
    "      threshold_high: Cosine similarity threshold for high marks (default 0.7).\n",
    "      threshold_medium: Cosine similarity threshold for medium marks (default 0.5).\n",
    "      marks_high: Marks awarded for high similarity (default 50).\n",
    "      marks_medium: Marks awarded for medium similarity (default 35).\n",
    "      marks_low: Marks awarded for low similarity (default 10).\n",
    "\n",
    "  Returns:\n",
    "      A modified DataFrame with a new column 'Marks_With_CV' containing marks.\n",
    "  \"\"\"\n",
    "  data['Marks_With_Cosine'] = data['Cosine_Similarity_Keywords'].apply(lambda score: \n",
    "                                                         marks_high if score > threshold_high else (\n",
    "                                                             marks_medium if score > threshold_medium else marks_low\n",
    "                                                         )\n",
    "                                                       )\n",
    "  return data\n",
    "\n",
    "# Assuming your DataFrame is called 'data'\n",
    "newdata = assign_marks_with_countvectorizer(newdata.copy())\n",
    "\n",
    "# Print the DataFrame with the new 'Marks_With_CV' column (optional)\n",
    "print(newdata)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc6871a",
   "metadata": {},
   "source": [
    "### <font color=\"red\"> FIND COSINE USING TF-IDF</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ccc4a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Assuming 'teacher_keywords' is a list of teacher answer keywords and 'student_comments' is a list of student comments\n",
    "teacher_keywords = ['variable', 'declaration', 'integer', 'int', 'declare', 'datatype', \"data\", \"type\"]\n",
    "student_comments = newdata[\"CorrectComment2_Spell\"]\n",
    "\n",
    "# Create TF-IDF vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the teacher keywords and transform the student comments using the TF-IDF vectorizer\n",
    "tfidf_matrix_keywords = tfidf_vectorizer.fit_transform([\" \".join(teacher_keywords)])\n",
    "tfidf_matrix_comments = tfidf_vectorizer.transform(student_comments)\n",
    "\n",
    "# Calculate cosine similarity between teacher keywords and student comments\n",
    "cosine_similarities = cosine_similarity(tfidf_matrix_keywords, tfidf_matrix_comments)\n",
    "\n",
    "# Store cosine similarity values in newdata dataframe\n",
    "newdata['Cosine_Similarity_Tfidf_Keywords'] = cosine_similarities.squeeze()\n",
    "\n",
    "# Print ID, Cosine_Similarity_Keywords, and Cosine_Similarity_Tfidf_Keywords\n",
    "# print(newdata[['ID', 'Cosine_Similarity_Keywords']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31423b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "newdata[[\"ID\",\"Cosine_Similarity_Keywords\",'Cosine_Similarity_Tfidf_Keywords']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbec47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "newdata[['Match_with_Answer_keywords2', 'Match_with_Answer_keywords3','Total_Answer_Keyword2']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe49f25e",
   "metadata": {},
   "source": [
    "### <font color=\"Blue\"> APPLY SENTIMENT ANALYSIS </font> [ NOT HELPFUL ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0fb896",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# Download the VADER lexicon\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# Initialize the VADER sentiment analyzer\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Perform sentiment analysis on each comment\n",
    "sentiment_scores = []\n",
    "for comment in newdata['CorrectComment3_Spell']:\n",
    "    # Calculate sentiment scores\n",
    "    scores = sid.polarity_scores(comment)\n",
    "    sentiment_scores.append(scores)\n",
    "\n",
    "# Extract compound scores\n",
    "compound_scores = [score['compound'] for score in sentiment_scores]\n",
    "\n",
    "# Classify comments based on sentiment scores\n",
    "classification = []\n",
    "for score in compound_scores:\n",
    "    if score > 0.05:  # Positive sentiment\n",
    "        classification.append(\"Solution Only\")\n",
    "    elif score < -0.05:  # Negative sentiment\n",
    "        classification.append(\"Error Only\")\n",
    "    else:  # Neutral sentiment\n",
    "        classification.append(\"Both Error and Solution\")\n",
    "\n",
    "# Add sentiment scores and classification to the DataFrame\n",
    "newdata['Sentiment_Score'] = compound_scores\n",
    "newdata['Sentiment_Classification'] = classification\n",
    "\n",
    "# Display the DataFrame with sentiment scores and classification\n",
    "print(newdata[['ID','CorrectComment3_Spell', 'Sentiment_Score', 'Sentiment_Classification']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abcc8cb6",
   "metadata": {},
   "source": [
    "#### SENTIMENT ANALYSIS : it is not much helpful to find comment category"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd1aada",
   "metadata": {},
   "source": [
    "### <font color=\"blue\"> CREATE MODEL AND APPLY ML ALGORITHM AND CHECK ACCURACY </FONT>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ce3422",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "\n",
    "# Assuming 'newdata' is your DataFrame containing the dataset with relevant features\n",
    "\n",
    "# Step 1: Prepare the data\n",
    "\n",
    "# Step 2: Create the target variable 'Mark_With_3Rule' based on features\n",
    "# Assuming 'Mark_With_3Rule' is already calculated based on rule-based method\n",
    "\n",
    "# Step 3: Preprocess the data for the 3-rule method\n",
    "encoder_3rule = OneHotEncoder(drop='first', sparse=False)\n",
    "X_encoded_3rule = encoder_3rule.fit_transform(newdata[['CommentType3', 'Comment_Category']].values)\n",
    "X_processed_3rule = np.concatenate([X_encoded_3rule, newdata[['Cosine_Similarity_Keywords', 'Total_Answer_Keyword2']].values], axis=1)\n",
    "y_3rule = newdata['Mark_With_3Rule']\n",
    "\n",
    "# Step 4: Split the data into training and testing sets for the 3-rule method\n",
    "X_train_3rule, X_test_3rule, y_train_3rule, y_test_3rule = train_test_split(X_processed_3rule, y_3rule, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 5: Define models for the 3-rule method\n",
    "models_3rule = {\n",
    "    \"Random Forest\": RandomForestRegressor(),\n",
    "    \"Gradient Boosting\": GradientBoostingRegressor()\n",
    "}\n",
    "\n",
    "# Step 6: Train and evaluate models for the 3-rule method\n",
    "results_3rule = {}\n",
    "for name, model in models_3rule.items():\n",
    "    model.fit(X_train_3rule, y_train_3rule)\n",
    "    y_pred_3rule = model.predict(X_test_3rule)\n",
    "    mse_3rule = mean_squared_error(y_test_3rule, y_pred_3rule)\n",
    "    results_3rule[name] = mse_3rule\n",
    "\n",
    "# Step 7: Select the best model for the 3-rule method\n",
    "best_model_name_3rule = min(results_3rule, key=results_3rule.get)\n",
    "best_model_3rule = models_3rule[best_model_name_3rule]\n",
    "print(\"Best Model for 3-rule method:\", best_model_name_3rule)\n",
    "print(\"Mean Squared Error on Test Set (3-rule method):\", results_3rule[best_model_name_3rule])\n",
    "\n",
    "# Repeat Steps 3-7 for the cosine method\n",
    "\n",
    "# Step 3: Preprocess the data for the cosine method\n",
    "# Assuming 'Mark_With_Cosine' is already calculated based on cosine similarity method\n",
    "\n",
    "# Step 4: Split the data into training and testing sets for the cosine method\n",
    "\n",
    "# Step 5: Define models for the cosine method\n",
    "\n",
    "# Step 6: Train and evaluate models for the cosine method\n",
    "\n",
    "# Step 7: Select the best model for the cosine method\n",
    "\n",
    "# Predict marks using the best model for the 3-rule method\n",
    "best_model_3rule.fit(X_processed_3rule, y_3rule)\n",
    "y_pred_3rule_best = best_model_3rule.predict(X_processed_3rule)\n",
    "newdata['Mark_With_ML_3Rule'] = y_pred_3rule_best\n",
    "\n",
    "# Repeat the prediction process for the cosine method\n",
    "\n",
    "# Display the results\n",
    "print(\"Best Model for 3-rule method:\", best_model_name_3rule)\n",
    "print(\"Mean Squared Error on Test Set (3-rule method):\", results_3rule[best_model_name_3rule])\n",
    "# Repeat for the cosine method\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58cbdac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "newdata[['Mark_With_3Rule', 'Marks_With_Cosine', 'Mark_With_ML_3Rule']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac43650",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "# Select relevant features and target variable\n",
    "X = newdata[['Comment_Length','Cosine_Similarity_Keywords','Total_Answer_Keyword2']]\n",
    "y = newdata['Mark_With_3Rule']\n",
    "\n",
    "# Encode categorical variables\n",
    "label_encoder = LabelEncoder()\n",
    "X['CommentType'] = label_encoder.fit_transform(newdata['Comment_Length'] > 50)\n",
    "X['CommentCategory'] = label_encoder.fit_transform(newdata['Comment_Category'])\n",
    "\n",
    "# Derive Total_Matching_Keywords\n",
    "# Convert columns to string and then split them to compute the total number of keywords\n",
    "# keyword_columns = ['Error_Matched_Keywords3', 'Solution_Matched_Keywords3', 'Both_Matched_Keywords3']\n",
    "# X['Total_Matching_Keywords'] = newdata[keyword_columns].astype(str).apply(lambda x: x.str.split(',').str.len()).sum(axis=1)\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale numerical features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(),\n",
    "    'Random Forest': RandomForestClassifier(),\n",
    "    'Support Vector Machines': SVC()\n",
    "}\n",
    "\n",
    "# Train and evaluate models\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    results[name] = accuracy\n",
    "\n",
    "# Compare results\n",
    "for name, accuracy in results.items():\n",
    "    print(f\"{name}: Accuracy = {accuracy:.4f}\")\n",
    "\n",
    "best_model = max(results, key=results.get)\n",
    "print(f\"\\nBest Model: {best_model}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19bc636",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Prepare the data\n",
    "# Assuming 'newdata' is your DataFrame containing the dataset with relevant features\n",
    "\n",
    "# Step 2: Encode categorical variables using one-hot encoding\n",
    "encoder = OneHotEncoder()\n",
    "encoded_data = encoder.fit_transform(newdata[['CommentType3', 'Comment_Category']])\n",
    "\n",
    "# Combine encoded features with numerical features\n",
    "X = np.concatenate([encoded_data.toarray(), newdata[['Cosine_Similarity_Keywords', 'Total_Answer_Keyword2']].values], axis=1)\n",
    "\n",
    "# Target variable for rule-based method\n",
    "y_3rule = newdata['Mark_With_3Rule']\n",
    "\n",
    "# Target variable for cosine similarity method\n",
    "y_cosine = newdata['Marks_With_Cosine']\n",
    "\n",
    "# Split the data into training and testing sets for 3-rule method\n",
    "X_train_3rule, X_test_3rule, y_train_3rule, y_test_3rule = train_test_split(X, y_3rule, test_size=0.2, random_state=42)\n",
    "\n",
    "# Split the data into training and testing sets for cosine similarity method\n",
    "X_train_cosine, X_test_cosine, y_train_cosine, y_test_cosine = train_test_split(X, y_cosine, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 3: Define models\n",
    "models = {\n",
    "    \"Random Forest\": RandomForestRegressor(),\n",
    "    \"Gradient Boosting\": GradientBoostingRegressor(),\n",
    "    \"Support Vector Machine\": SVR(),\n",
    "    \"Linear Regression\": LinearRegression()\n",
    "}\n",
    "\n",
    "# Step 4: Train and evaluate models for 3-rule method\n",
    "results_3rule = {}\n",
    "for name, model in models.items():\n",
    "    # Train the model\n",
    "    model.fit(X_train_3rule, y_train_3rule)\n",
    "    # Evaluate model performance\n",
    "    y_pred_3rule = model.predict(X_test_3rule)\n",
    "    mse_3rule = mean_squared_error(y_test_3rule, y_pred_3rule)\n",
    "    results_3rule[name] = mse_3rule\n",
    "\n",
    "# Step 5: Train and evaluate models for cosine similarity method\n",
    "results_cosine = {}\n",
    "for name, model in models.items():\n",
    "    # Train the model\n",
    "    model.fit(X_train_cosine, y_train_cosine)\n",
    "    # Evaluate model performance\n",
    "    y_pred_cosine = model.predict(X_test_cosine)\n",
    "    mse_cosine = mean_squared_error(y_test_cosine, y_pred_cosine)\n",
    "    results_cosine[name] = mse_cosine\n",
    "\n",
    "# Step 6: Select the best model for 3-rule method\n",
    "best_model_3rule_name = min(results_3rule, key=results_3rule.get)\n",
    "best_model_3rule = models[best_model_3rule_name]\n",
    "print(\"Best Model for 3-rule method:\", best_model_3rule_name)\n",
    "print(\"Mean Squared Error on Test Set for 3-rule method:\", results_3rule[best_model_3rule_name])\n",
    "\n",
    "# Step 7: Select the best model for cosine similarity method\n",
    "best_model_cosine_name = min(results_cosine, key=results_cosine.get)\n",
    "best_model_cosine = models[best_model_cosine_name]\n",
    "print(\"Best Model for cosine similarity method:\", best_model_cosine_name)\n",
    "print(\"Mean Squared Error on Test Set for cosine similarity method:\", results_cosine[best_model_cosine_name])\n",
    "\n",
    "# Step 8: Predict marks using the best models and store them in new columns\n",
    "# best_model_3rule.fit(X, y_3rule)\n",
    "# best_model_cosine.fit(X, y_cosine)\n",
    "\n",
    "# newdata['Mark_With_ML_3Rule'] = best_model_3rule.predict(X)\n",
    "# newdata['Mark_With_ML_Cosine'] = best_model_cosine.predict(X)\n",
    "# Step 8: Predict marks using all models and store them in new columns\n",
    "for name, model in models.items():\n",
    "    newdata[f'Mark_With_ML_{name.replace(\" \", \"_\")}'] = model.predict(X)\n",
    "\n",
    "# Print the marks predicted by all algorithms\n",
    "for name, model in models.items():\n",
    "    predicted_marks = newdata[f'Mark_With_ML_{name.replace(\" \", \"_\")}']\n",
    "    print(f\"Predicted marks by {name}: {predicted_marks}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ce2c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "newdata.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edae1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "newdata[['Mark_With_4Rule',\n",
    "       'Mark_With_3Rule', 'Marks_With_Cosine', 'Mark_With_ML_3Rule',\n",
    "       'Mark_With_ML_Cosine', 'Mark_With_ML_Random_Forest',\n",
    "       'Mark_With_ML_Gradient_Boosting', 'Mark_With_ML_Support_Vector_Machine',\n",
    "       'Mark_With_ML_Linear_Regression']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47bc3f6e",
   "metadata": {},
   "source": [
    "- Based on above result , we said that \"Random Forest\" algorithm gives best result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c78eaa",
   "metadata": {},
   "source": [
    "### <FONT COLOR=\"BLUE\"> APPLY ML USING RULE BASED METHOD MARKS</FONT>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ff35cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Prepare the data\n",
    "# Assuming 'newdata' is your DataFrame containing the dataset with relevant features\n",
    "\n",
    "# Step 2: Create the target variable 'Mark' based on the rule-based method\n",
    "# For demonstration purposes, let's say you assign marks based on some logic\n",
    "# Replace this logic with your actual method to assign marks\n",
    "newdata['Mark_With_Rule'] = ...  # Assign marks based on the rule-based method\n",
    "\n",
    "# Step 3: Define the features and target variable\n",
    "X = newdata[['Cosine_Similarity_Keywords', 'CommentType3', 'Total_Answer_Keyword2', 'Comment_Category']]\n",
    "y = newdata['Mark_With_3Rule']\n",
    "\n",
    "# Step 4: Encode categorical variables using one-hot encoding\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "encoder = OneHotEncoder(drop='first', sparse=False)\n",
    "X_encoded = encoder.fit_transform(X[['CommentType3', 'Comment_Category']])\n",
    "\n",
    "# Combine encoded features with numerical features\n",
    "X_processed = np.concatenate([X_encoded, X.drop(columns=['CommentType3', 'Comment_Category']).values], axis=1)\n",
    "\n",
    "# Step 5: Split the data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_processed, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 6: Define ML models\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "models = {\n",
    "    \"Random Forest\": RandomForestRegressor(),\n",
    "    \"Gradient Boosting\": GradientBoostingRegressor(),\n",
    "    \"Linear Regression\": LinearRegression(),\n",
    "    \"SVR\": SVR()\n",
    "}\n",
    "\n",
    "# Step 7: Train and evaluate ML models\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    results[name] = mse\n",
    "\n",
    "# Step 8: Select the best model\n",
    "best_model_name = min(results, key=results.get)\n",
    "best_model = models[best_model_name]\n",
    "print(\"Best Model for Rule-Based Method:\", best_model_name)\n",
    "print(\"Mean Squared Error on Test Set for Rule-Based Method:\", results[best_model_name])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55dca181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Prepare the data\n",
    "# Assuming 'newdata' is your DataFrame containing the dataset with relevant features\n",
    "\n",
    "# Step 2: Create the target variable 'Mark' based on the rule-based method\n",
    "# For demonstration purposes, let's say you assign marks based on some logic\n",
    "# Replace this logic with your actual method to assign marks\n",
    "newdata['Mark_With_Rule'] = ...  # Assign marks based on the rule-based method\n",
    "\n",
    "# Step 3: Define the features and target variable\n",
    "X = newdata[['Cosine_Similarity_Keywords', 'CommentType3', 'Total_Answer_Keyword2', 'Comment_Category']]\n",
    "y = newdata['Mark_With_3Rule']\n",
    "\n",
    "# Step 4: Encode categorical variables using one-hot encoding\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "encoder = OneHotEncoder(drop='first', sparse=False)\n",
    "X_encoded = encoder.fit_transform(X[['CommentType3', 'Comment_Category']])\n",
    "\n",
    "# Combine encoded features with numerical features\n",
    "X_processed = np.concatenate([X_encoded, X.drop(columns=['CommentType3', 'Comment_Category']).values], axis=1)\n",
    "\n",
    "# Step 5: Split the data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_processed, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 6: Define ML models\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "models = {\n",
    "    \"Random Forest\": RandomForestRegressor(),\n",
    "    \"Gradient Boosting\": GradientBoostingRegressor(),\n",
    "    \"Linear Regression\": LinearRegression(),\n",
    "    \"SVR\": SVR()\n",
    "}\n",
    "\n",
    "# Step 7: Train and evaluate ML models\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    results[name] = mse\n",
    "\n",
    "\n",
    "\n",
    "# Step 8: Train and evaluate ML models\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "results = {}\n",
    "predicted_marks = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    results[name] = mse\n",
    "    predicted_marks[name] = y_pred\n",
    "\n",
    "# Step 9: Print the predicted marks of all algorithms\n",
    "print(\"Predicted Marks by Rule-Based Method:\")\n",
    "print(y_test.values)  # Actual marks from the test set\n",
    "print(\"\\nPredicted Marks by ML Algorithms:\")\n",
    "for name, marks in predicted_marks.items():\n",
    "    print(f\"{name}: {marks}\")\n",
    "\n",
    "# Step 10: Select the best model\n",
    "best_model_name = min(results, key=results.get)\n",
    "best_model = models[best_model_name]\n",
    "print(\"\\nBest Model for Rule-Based Method:\", best_model_name)\n",
    "print(\"Mean Squared Error on Test Set for Rule-Based Method:\", results[best_model_name])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ee6f55",
   "metadata": {},
   "source": [
    "### <FONT COLOR=\"BLUE\"> APPLY RANDOM FOREST AND ASSIGN MARK </FONT>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252cd105",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "# Select relevant features and target variable\n",
    "X = newdata[['Cosine_Similarity_Keywords', 'CommentType3', 'Total_Answer_Keyword2', 'Comment_Category']]\n",
    "y = newdata['Mark_With_3Rule']\n",
    "\n",
    "# Step 4: Encode categorical variables using one-hot encoding\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "encoder = OneHotEncoder(drop='first', sparse=False)\n",
    "X_encoded = encoder.fit_transform(X[['CommentType3', 'Comment_Category']])\n",
    "\n",
    "# Combine encoded features with numerical features\n",
    "X_processed = np.concatenate([X_encoded, X.drop(columns=['CommentType3', 'Comment_Category']).values], axis=1)\n",
    "\n",
    "# # Scale numerical features\n",
    "# scaler = StandardScaler()\n",
    "# X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Train Random Forest model on the entire dataset\n",
    "rf_model = RandomForestRegressor()\n",
    "rf_model.fit(X_processed, y)\n",
    "\n",
    "# Predict marks for each student\n",
    "predicted_marks = rf_model.predict(X_processed)\n",
    "\n",
    "# Add predicted marks to the DataFrame\n",
    "newdata['Predicted_Marks'] = predicted_marks\n",
    "\n",
    "# Display the DataFrame with predicted marks\n",
    "print(newdata[[\"ID\",'Name', 'Mark_With_3Rule','Predicted_Marks']])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db79ebcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "# from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "# # Select relevant features and target variable\n",
    "# X = newdata[['Cosine_Similarity_Keywords', 'CommentType3', 'Total_Answer_Keyword2', 'Comment_Category']]\n",
    "# y = newdata['Marks_With_Cosine']\n",
    "\n",
    "# # Step 4: Encode categorical variables using one-hot encoding\n",
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "# encoder = OneHotEncoder(drop='first', sparse=False)\n",
    "# X_encoded = encoder.fit_transform(X[['CommentType3', 'Comment_Category']])\n",
    "\n",
    "# # Combine encoded features with numerical features\n",
    "# X_processed = np.concatenate([X_encoded, X.drop(columns=['CommentType3', 'Comment_Category']).values], axis=1)\n",
    "\n",
    "# # # Scale numerical features\n",
    "# # scaler = StandardScaler()\n",
    "# # X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# # Train Random Forest model on the entire dataset\n",
    "# rf_model = RandomForestRegressor()\n",
    "# rf_model.fit(X_processed, y)\n",
    "\n",
    "# # Predict marks for each student\n",
    "# predicted_marks = rf_model.predict(X_processed)\n",
    "\n",
    "# # Add predicted marks to the DataFrame\n",
    "# newdata['Predicted_Marks_Cos_RF'] = predicted_marks\n",
    "\n",
    "# # Display the DataFrame with predicted marks\n",
    "# # print(newdata[[\"ID\",'Name', 'Mark_With_3Rule','Marks_With_Cosine','Predicted_Marks']])\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "\n",
    "# Select relevant features and target variable\n",
    "X = newdata[['Cosine_Similarity_Keywords', 'CommentType3', 'Total_Answer_Keyword2', 'Comment_Category']]\n",
    "y_cosine = newdata['Marks_With_Cosine']\n",
    "\n",
    "# Step 4: Encode categorical variables using one-hot encoding\n",
    "encoder = OneHotEncoder(drop='first', sparse=False)\n",
    "X_encoded = encoder.fit_transform(X[['CommentType3', 'Comment_Category']])\n",
    "\n",
    "# Combine encoded features with numerical features\n",
    "X_processed = np.concatenate([X_encoded, X.drop(columns=['CommentType3', 'Comment_Category']).values], axis=1)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_processed, y_cosine, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Random Forest model on the training set\n",
    "rf_model = RandomForestRegressor()\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict marks for the testing set\n",
    "predicted_marks_rf = rf_model.predict(X_test)\n",
    "# newdata['predicted_marks_RF']=predicted_marks_rf\n",
    "# Compare the predicted marks with the marks generated by cosine similarity\n",
    "comparison_df = pd.DataFrame({'Predicted_Marks_RF': predicted_marks_rf, 'Marks_With_Cosine': y_test})\n",
    "print(comparison_df)\n",
    "print(len(comparison_df))\n",
    "\n",
    "# Extract true marks from the DataFrame\n",
    "y_true = newdata['Marks_With_Cosine']\n",
    "\n",
    "# Compute ML-generated marks using the trained model\n",
    "predicted_marks_ml = rf_model.predict(X_processed)\n",
    "\n",
    "# Compare ML-generated marks with cosine similarity-generated marks\n",
    "mse_ml = mean_squared_error(y_true, predicted_marks_ml)\n",
    "mae_ml = mean_absolute_error(y_true, predicted_marks_ml)\n",
    "r2_ml = r2_score(y_true, predicted_marks_ml)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"Mean Squared Error (ML):\", mse_ml)\n",
    "print(\"Mean Absolute Error (ML):\", mae_ml)\n",
    "print(\"R^2 Score (ML):\", r2_ml)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d9dd6c",
   "metadata": {},
   "source": [
    "### CHECK ACCURACY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80255fb8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Prepare the data\n",
    "# Extract features and target variables from rule-based method\n",
    "X_rule = newdata[['CommentType3', 'Total_Answer_Keyword2', 'Comment_Category']]\n",
    "y_rule = newdata['Mark_With_3Rule']\n",
    "\n",
    "# Extract features and target variables from cosine similarity method\n",
    "X_cosine = newdata[['Cosine_Similarity_Keywords', 'CommentType3', 'Total_Answer_Keyword2', 'Comment_Category']]\n",
    "y_cosine = newdata['Marks_With_Cosine']\n",
    "\n",
    "# Step 2: Combine features from both methods\n",
    "# Rule-based method features\n",
    "encoder = OneHotEncoder(drop='first', sparse=False)\n",
    "X_rule_encoded = encoder.fit_transform(X_rule[['CommentType3', 'Comment_Category']])\n",
    "X_rule_processed = np.concatenate([X_rule_encoded, X_rule.drop(columns=['CommentType3', 'Comment_Category'])], axis=1)\n",
    "\n",
    "# Cosine similarity method features\n",
    "X_cosine_encoded = encoder.transform(X_cosine[['CommentType3', 'Comment_Category']])\n",
    "X_cosine_processed = np.concatenate([X_cosine_encoded, X_cosine.drop(columns=['CommentType3', 'Comment_Category'])], axis=1)\n",
    "\n",
    "# Combine features from both methods\n",
    "X_combined = np.concatenate([X_rule_processed, X_cosine_processed], axis=1)\n",
    "y_combined = y_rule  # Use marks from the rule-based method as target variable for training\n",
    "\n",
    "# Step 3: Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_combined, y_combined, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 4: Train a Random Forest regressor\n",
    "rf_model = RandomForestRegressor()\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Step 5: Predict marks using the trained model\n",
    "predicted_marks = rf_model.predict(X_test)\n",
    "\n",
    "# Step 6: Print actual and predicted marks\n",
    "print(\"Actual Marks:\", y_test,\"Predicted Marks:\", predicted_marks)\n",
    "\n",
    "# Step 7: Evaluate the performance of the model\n",
    "mse = mean_squared_error(y_test, predicted_marks)\n",
    "mae = mean_absolute_error(y_test, predicted_marks)\n",
    "r2 = r2_score(y_test, predicted_marks)\n",
    "\n",
    "print(\"Mean Squared Error (ML):\", mse)\n",
    "print(\"Mean Absolute Error (ML):\", mae)\n",
    "print(\"R^2 Score (ML):\", r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10a3628",
   "metadata": {},
   "outputs": [],
   "source": [
    "newdata[[\"ID\",'Name', 'Mark_With_3Rule','Predicted_Marks','Marks_With_Cosine','Predicted_Marks_Cos_RF']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5a93c1",
   "metadata": {},
   "source": [
    "### <FONT COLOR=\"BLUE\">FIND THE DIFFERENCE BETWEEN MARK AND RANDOM FOREST PREDICTED MARKS </FONT>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44dc870",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "# Select relevant features and target variable\n",
    "X = newdata[['Cosine_Similarity_Keywords', 'CommentType3', 'Total_Answer_Keyword2', 'Comment_Category']]\n",
    "y = newdata['Mark_With_3Rule']\n",
    "\n",
    "\n",
    "# Encode categorical variables\n",
    "label_encoder = LabelEncoder()\n",
    "X['CommentType3'] = label_encoder.fit_transform(X['CommentType3'])\n",
    "X['Comment_Category'] = label_encoder.fit_transform(X['Comment_Category'])\n",
    "# Scale numerical features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Train Random Forest model on the entire dataset\n",
    "rf_model = RandomForestRegressor()\n",
    "rf_model.fit(X_scaled, y)\n",
    "\n",
    "# Predict marks for each student\n",
    "predicted_marks = rf_model.predict(X_scaled)\n",
    "\n",
    "# Add predicted marks to the DataFrame\n",
    "newdata['Predicted_Marks'] = predicted_marks\n",
    "\n",
    "# Compare predicted marks with manually calculated marks\n",
    "newdata['Difference'] = newdata['Mark_With_3Rule'] - newdata['Predicted_Marks']\n",
    "\n",
    "# Write DataFrame to CSV file\n",
    "csv_file_path = 'G:\\\\GitHub Classroom Assistant\\\\Topic Modelling Dataset\\\\TMC\\\\predicted_marks_comparison.csv'\n",
    "newdata.to_csv(csv_file_path, index=False)\n",
    "\n",
    "# Display the DataFrame with predicted marks and difference\n",
    "print(newdata[['Name', 'Mark_With_3Rule', 'Predicted_Marks', 'Difference']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1de5404",
   "metadata": {},
   "source": [
    "### <font color=\"blue\"> APPLY GRADIENT BOOSTING ALGORITHM </FONT>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9fd1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "\n",
    "\n",
    "# Select features and target variable\n",
    "X = newdata[['Cosine_Similarity_Keywords', 'CommentType3', 'Total_Answer_Keyword2', 'Comment_Category']]\n",
    "y = newdata['Mark_With_3Rule']\n",
    "\n",
    "# Encode categorical variables\n",
    "label_encoder = LabelEncoder()\n",
    "X['CommentType3'] = label_encoder.fit_transform(X['CommentType3'])\n",
    "X['Comment_Category'] = label_encoder.fit_transform(X['Comment_Category'])\n",
    "# Scale numerical features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split the dataset into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize Gradient Boosting Regressor\n",
    "gb_model = GradientBoostingRegressor()\n",
    "\n",
    "# Train the model\n",
    "gb_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict marks on the test set\n",
    "y_pred = gb_model.predict(X_test)\n",
    "\n",
    "# Print the predicted marks\n",
    "print(\"Predicted Marks with Gradient Boosting Algorithm:\")\n",
    "for i, pred_mark in enumerate(y_pred):\n",
    "    print(f\"Student {i+1}: Predicted Mark = {pred_mark}\")\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"\\nAccuracy Metrics:\")\n",
    "print(\"Mean Squared Error (GB):\", mse)\n",
    "print(\"Mean Absolute Error (GB):\", mae)\n",
    "print(\"R^2 Score (GB):\", r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3c0684",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c400b228",
   "metadata": {},
   "outputs": [],
   "source": [
    "newdata[[\"CleanComment3_Length\",'CleanComment4_Length']]=data[[\"CleanComment3_Length\",'CleanComment4_Length']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be968d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "newdata.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b18085",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# Select relevant features and target variable\n",
    "X = newdata[['Comment_Length', 'Cosine_Similarity_Keywords', 'CommentType3', 'Total_Answer_Keyword2']]\n",
    "y = newdata['Mark']\n",
    "\n",
    "# Encode categorical variables\n",
    "label_encoder = LabelEncoder()\n",
    "X['CommentType3'] = label_encoder.fit_transform(X['CommentType3'])\n",
    "\n",
    "# Scale numerical features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Train Random Forest model on the entire dataset\n",
    "rf_model = RandomForestRegressor()\n",
    "rf_model.fit(X_scaled, y)\n",
    "\n",
    "# Predict marks for each student\n",
    "predicted_marks = rf_model.predict(X_scaled)\n",
    "\n",
    "# Take the ceiling value of predicted marks\n",
    "ceiling_predicted_marks = np.ceil(predicted_marks)\n",
    "\n",
    "# Add predicted marks to the DataFrame\n",
    "newdata['Predicted_Marks'] = ceiling_predicted_marks\n",
    "\n",
    "# Compare predicted marks with manually calculated marks\n",
    "newdata['Difference'] = newdata['Mark'] - newdata['Predicted_Marks']\n",
    "\n",
    "# Write DataFrame to CSV file\n",
    "csv_file_path = 'G:\\\\GitHub Classroom Assistant\\\\Topic Modelling Dataset\\\\TMC\\\\predicted_marks_comparison.csv'\n",
    "newdata.to_csv(csv_file_path, index=False)\n",
    "\n",
    "# Display the DataFrame with predicted marks and difference\n",
    "print(newdata[['Name', 'Mark', 'Predicted_Marks', 'Difference']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fcb2583",
   "metadata": {},
   "source": [
    "### FIND ACCURACY BETWEEN MARKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2c9294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a threshold for considering a prediction as correct\n",
    "threshold = 1\n",
    "\n",
    "# Calculate the absolute difference between Mark and Predicted_Marks\n",
    "newdata['Absolute_Difference'] = abs(newdata['Mark_With_3Rule'] - newdata['Predicted_Marks'])\n",
    "\n",
    "# Count the number of correct predictions\n",
    "correct_predictions = (newdata['Absolute_Difference'] <= threshold).sum()\n",
    "\n",
    "# Calculate the accuracy\n",
    "total_samples = len(newdata)\n",
    "accuracy = correct_predictions / total_samples\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d3364a",
   "metadata": {},
   "source": [
    "### <FONT COLOR=\"BLUE\"> APPLY SVM & REGRESSION MODEL TO PREDICT MARK </FONT>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d0cdd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Train SVM model\n",
    "svm_model = SVR()\n",
    "svm_model.fit(X_scaled, y)\n",
    "\n",
    "# Predict marks using SVM\n",
    "svm_predicted_marks = svm_model.predict(X_scaled)\n",
    "\n",
    "# Train Regression model\n",
    "regression_model = LinearRegression()\n",
    "regression_model.fit(X_scaled, y)\n",
    "\n",
    "# Predict marks using Regression\n",
    "regression_predicted_marks = regression_model.predict(X_scaled)\n",
    "\n",
    "# Add predicted marks to the DataFrame\n",
    "newdata['SVM_Predicted_Marks'] = svm_predicted_marks\n",
    "newdata['Regression_Predicted_Marks'] = regression_predicted_marks\n",
    "\n",
    "# Calculate the accuracy for each model\n",
    "threshold = 1\n",
    "newdata['SVM_Absolute_Difference'] = abs(newdata['Mark'] - newdata['SVM_Predicted_Marks'])\n",
    "newdata['Regression_Absolute_Difference'] = abs(newdata['Mark'] - newdata['Regression_Predicted_Marks'])\n",
    "\n",
    "svm_correct_predictions = (newdata['SVM_Absolute_Difference'] <= threshold).sum()\n",
    "regression_correct_predictions = (newdata['Regression_Absolute_Difference'] <= threshold).sum()\n",
    "\n",
    "svm_accuracy = svm_correct_predictions / total_samples\n",
    "regression_accuracy = regression_correct_predictions / total_samples\n",
    "\n",
    "print(\"SVM Accuracy:\", svm_accuracy)\n",
    "print(\"Regression Accuracy:\", regression_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03d2e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "newdata[['Mark', 'Predicted_Marks', 'SVM_Predicted_Marks', 'Regression_Predicted_Marks']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22292f8d",
   "metadata": {},
   "source": [
    "### <FONT COLOR=\"BLUE\"> FIND ACCURACY BASED ON MARKS </FONT>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09091ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to calculate accuracy based on absolute difference\n",
    "def calculate_accuracy(true_marks, predicted_marks):\n",
    "    threshold = 1  # Define the threshold for considering a prediction as correct\n",
    "    absolute_difference = abs(true_marks - predicted_marks)\n",
    "    correct_predictions = (absolute_difference <= threshold).sum()\n",
    "    total_samples = len(true_marks)\n",
    "    accuracy = correct_predictions / total_samples\n",
    "    return accuracy\n",
    "\n",
    "# Calculate accuracy for Random Forest\n",
    "rf_accuracy = calculate_accuracy(newdata['Mark'], newdata['Predicted_Marks'])\n",
    "\n",
    "# Calculate accuracy for SVM\n",
    "svm_accuracy = calculate_accuracy(newdata['Mark'], newdata['SVM_Predicted_Marks'])\n",
    "\n",
    "# Calculate accuracy for Regression\n",
    "regression_accuracy = calculate_accuracy(newdata['Mark'], newdata['Regression_Predicted_Marks'])\n",
    "\n",
    "# Print accuracy values\n",
    "print(\"Accuracy of Random Forest:\", rf_accuracy)\n",
    "print(\"Accuracy of SVM:\", svm_accuracy)\n",
    "print(\"Accuracy of Regression:\", regression_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b764f38d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7907c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Define a function to calculate accuracy based on absolute difference\n",
    "def calculate_accuracy(true_marks, predicted_marks):\n",
    "    threshold = 1  # Define the threshold for considering a prediction as correct\n",
    "    absolute_difference = abs(true_marks - predicted_marks)\n",
    "    correct_predictions = (absolute_difference <= threshold).sum()\n",
    "    total_samples = len(true_marks)\n",
    "    accuracy = correct_predictions / total_samples\n",
    "    return accuracy\n",
    "\n",
    "# Calculate accuracy for Random Forest\n",
    "rf_accuracy = calculate_accuracy(newdata['Mark'], newdata['Predicted_Marks'])\n",
    "\n",
    "# Calculate other evaluation metrics for Random Forest\n",
    "rf_precision = precision_score(newdata['Mark'], newdata['Predicted_Marks'], average='macro')\n",
    "rf_recall = recall_score(newdata['Mark'], newdata['Predicted_Marks'], average='macro')\n",
    "rf_f1 = f1_score(newdata['Mark'], newdata['Predicted_Marks'], average='macro')\n",
    "\n",
    "# Print evaluation metrics for Random Forest\n",
    "print(\"Evaluation metrics for Random Forest:\")\n",
    "print(\"Accuracy:\", rf_accuracy)\n",
    "print(\"Precision:\", rf_precision)\n",
    "print(\"Recall:\", rf_recall)\n",
    "print(\"F1 Score:\", rf_f1)\n",
    "\n",
    "# Similarly, calculate evaluation metrics for SVM and Regression models\n",
    "# Modify the code accordingly for SVM and Regression models\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3418b8a3",
   "metadata": {},
   "source": [
    "### <font color=\"blue\"> FIND SIMILARITY USING JACCARD, BERT AND OTHER METHOD </FONT>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35772a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "newdata.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b11d38c",
   "metadata": {},
   "source": [
    "### <font color=\"Blue\"> APPLY ML ALGORITHM TO FIND COMMENT IS BASIC OR DETAIL </FONT>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31b938e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the labeled dataset\n",
    "data = pd.read_csv(\"G:\\\\GitHub Classroom Assistant\\\\Topic Modelling Dataset\\\\TMC\\\\predicted_marks_comparison.csv\")  # Assuming you have a CSV file with labeled data\n",
    "\n",
    "# Prepare features and target variable\n",
    "X = data[['CleanComment3_Length']]  # Assuming 'Comment_Length' is a column containing comment lengths\n",
    "y = data['CommentType3']  # Assuming 'CommentType' is a column containing comment types (basic or detail)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the Logistic Regression model\n",
    "model_comment_type = LogisticRegression()\n",
    "model_comment_type.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model_comment_type.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Store predictions in a new column\n",
    "data['Pred_CommentType'] = model_comment_type.predict(data[['CleanComment3_Length']])\n",
    "print((data[\"Pred_CommentType\"]))\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa456f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "# Load the labeled dataset\n",
    "data = pd.read_csv(\"G:\\\\GitHub Classroom Assistant\\\\Topic Modelling Dataset\\\\TMC\\\\predicted_marks_comparison.csv\")  # Assuming you have a CSV file with labeled data\n",
    "\n",
    "# Prepare features and target variable\n",
    "X = data[['CleanComment3_Length']]  # Assuming 'Comment_Length' is a column containing comment lengths\n",
    "y = data['CommentType3']  # Assuming 'CommentType' is a column containing comment types (basic or detail)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train and evaluate multiple models\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"Random Forest\": RandomForestClassifier()\n",
    "}\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"{name} Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6a2a23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8423aa88",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfe89e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[[\"ID\",\"CleanComment3\",\"CommentType3\",'Pred_CommentType']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78f1f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with ID, clean comment, actual comment type, and predicted comment type\n",
    "results = data[['ID', 'CleanComment3', 'CommentType3', 'Pred_CommentType']]\n",
    "\n",
    "# Filter out rows where the predicted comment type does not match the actual comment type\n",
    "incorrect_predictions = results[results['CommentType3'] != results['Pred_CommentType']]\n",
    "\n",
    "# Print the DataFrame containing incorrect predictions\n",
    "print(incorrect_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04cef177",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Pred_CommentType'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da69f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Step 1: Split the dataset into features (X) and target variable (y)\n",
    "X = newdata[['CleanComment3_Length', 'Total_Answer_Keyword2', 'Cosine_Similarity_Keywords', 'Comment_Category']]\n",
    "y = newdata['Mark']\n",
    "\n",
    "# Step 2: Encode categorical variable 'Comment_Category' using one-hot encoding\n",
    "encoder = OneHotEncoder()\n",
    "X_cat = encoder.fit_transform(newdata[['Comment_Category']])\n",
    "\n",
    "# Step 3: Combine encoded categorical variable and other numerical features\n",
    "X_numerical = newdata[['CleanComment3_Length', 'Total_Answer_Keyword2', 'Cosine_Similarity_Keywords']]\n",
    "X = np.concatenate([X_numerical, X_cat.toarray()], axis=1)\n",
    "\n",
    "# Step 4: Split the dataset into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 5: Train the machine learning model on the training set\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Step 6: Evaluate the model on the validation set\n",
    "y_pred = model.predict(X_val)\n",
    "mse = mean_squared_error(y_val, y_pred)\n",
    "print(\"Mean Squared Error on Validation Set:\", mse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e860bb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Calculate baseline prediction (mean or median)\n",
    "baseline_prediction = y_train.mean()  # Or use y_train.median() for median\n",
    "\n",
    "# Step 2: Create an array of baseline predictions for validation set\n",
    "baseline_predictions_val = np.full_like(y_val, fill_value=baseline_prediction)\n",
    "\n",
    "# Step 3: Calculate Mean Squared Error for baseline predictions\n",
    "baseline_mse = mean_squared_error(y_val, baseline_predictions_val)\n",
    "\n",
    "# Step 4: Compare baseline MSE with model MSE\n",
    "print(\"Baseline Mean Squared Error:\", baseline_mse)\n",
    "print(\"Model Mean Squared Error:\", mse)  # This is the MSE you calculated previously\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6b5a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Sample categorical data\n",
    "categories = ['red', 'blue', 'green', 'red', 'green']\n",
    "\n",
    "# Create an instance of OneHotEncoder\n",
    "encoder = OneHotEncoder()\n",
    "\n",
    "# Fit and transform the categorical data\n",
    "encoded_data = encoder.fit_transform(np.array(categories).reshape(-1, 1))\n",
    "\n",
    "# Convert the encoded data to a dense array\n",
    "encoded_data_dense = encoded_data.toarray()\n",
    "\n",
    "# Print the encoded data\n",
    "print(encoded_data_dense)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8de1514",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Assuming X contains input features (including precomputed features) and y contains target variable (Mark)\n",
    "# X should be a DataFrame containing features, and y should be a Series containing the target variable\n",
    "# You can use train_test_split to split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the machine learning model\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_pred = model.predict(X_val)\n",
    "\n",
    "# Evaluate the model using Mean Squared Error (MSE)\n",
    "mse = mean_squared_error(y_val, y_pred)\n",
    "print(\"Mean Squared Error on Validation Set:\", mse)\n",
    "\n",
    "print(\"Fail to reject the null hypothesis: There is no significant difference between the marks.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9688ed28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Assuming X contains input features (including precomputed features) and y contains target variable (Mark)\n",
    "# X should be a DataFrame containing features, and y should be a Series containing the target variable\n",
    "\n",
    "# Step 1: Split the dataset into features (X) and target variable (y)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 2: Initialize models\n",
    "models = {\n",
    "    \"Random Forest\": RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "    \"Gradient Boosting\": GradientBoostingRegressor(random_state=42),\n",
    "    \"Linear Regression\": LinearRegression(),\n",
    "    \"Support Vector Machine\": SVR(),\n",
    "    \"K-Nearest Neighbors\": KNeighborsRegressor()\n",
    "}\n",
    "\n",
    "# Step 3: Train and evaluate each model\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_val)\n",
    "    mse = mean_squared_error(y_val, y_pred)\n",
    "    print(f\"{name} Mean Squared Error on Validation Set: {mse}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c19290",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Step 3: Split the dataset into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 4: Initialize models\n",
    "models = {\n",
    "    \"Random Forest\": RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "    \"Gradient Boosting\": GradientBoostingRegressor(random_state=42),\n",
    "    \"Linear Regression\": LinearRegression(),\n",
    "    \"Support Vector Machine\": SVR(),\n",
    "    \"K-Nearest Neighbors\": KNeighborsRegressor()\n",
    "}\n",
    "\n",
    "# Step 5: Train and evaluate each model\n",
    "for name, model in models.items():\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict marks for the validation set\n",
    "    y_pred = model.predict(X_val)\n",
    "    \n",
    "    # Store predicted marks in a separate column\n",
    "    X_val[f\"{name}_Predict_Mark\"] = y_pred\n",
    "\n",
    "    # Evaluate model performance (optional)\n",
    "    mse = mean_squared_error(y_val, y_pred)\n",
    "    print(f\"{name} Mean Squared Error on Validation Set: {mse}\")\n",
    "\n",
    "# Optionally, you can save the modified DataFrame with predicted marks to a CSV file\n",
    "# X_val.to_csv('predicted_marks.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f63d80d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
